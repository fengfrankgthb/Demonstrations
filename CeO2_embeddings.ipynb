{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fengfrankgthb/Demonstrations/blob/main/CeO2_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2c6PyqQaNiA"
      },
      "source": [
        "# <font color=\"blue\">SAT & GRE tested w/ all-mpnet-base-v2 Model 2025.05.18 </font>\n",
        "\n",
        "<font color=\"blue\"> In this project, an well-trained **transformer model** is used to represent an knowledgable super-student in front of SAT and GRE reading quexstions. **Biases** and **confusions** are disclosed, likely attributable to **over-fitting** to the training of the model. Over-fitting is the ML terminology for **test cramming**, a phenominon when fitting specificities of **training dataset** caused the model to not being able to fit to specificities of **testing dataset**. </font>\n",
        "\n",
        "## <font color=\"blue\">1. Install Necessary Libraries\n",
        "* **sentence-transformers** This is the text embedding library\n",
        "* **scikit-learn** This is the machine learning library\n",
        "* **matplotlib** This is the mat-lab style plotting library\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This installation needs ~2min.\n",
        "!pip install sentence-transformers scikit-learn matplotlib"
      ],
      "metadata": {
        "id": "158mSJto-mYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color=\"blue\">2. import necessary modules from the libraries </font>\n",
        "\n"
      ],
      "metadata": {
        "id": "jd2Yqvua_ktp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color=\"blue\">**mpl_toolkits.mplot3d**: matplot 3D plotting lib\n",
        "* **numpy**: Numerical Python, the fundamental python lib\n",
        "* **Axes3D** 3D plotting class\n",
        "* **PCA** Principal Components Analysis for linear dimension reduction.\n",
        "* **TSNE** t-SNE non-linear dimension reduction to creat more scattered effect </font>"
      ],
      "metadata": {
        "id": "XPORFA-auaqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These imports need ~1min.\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "# set matplot to inline (static) mode, or notebook for interactive mode)\n",
        "# even though default is inline mode, be explicit to avoid any confusion\n",
        "# the interactive notebook mode is often unstable at colab environment\n",
        "# alternative is set as 'inline' and use 'plotly'.\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "d8ArIZFq_uxp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color=\"blue\">3. Imput text data</font>\n",
        "\n",
        "<font color=\"blue\">Choose one subsection below (3.1 ~ 3.2.4):</font>\n",
        "\n",
        "###  <font color=\"blue\">3.1 Example 1: CeO2-NPs, All-Mighty vs 5Vs (Section 8)</font>\n",
        "\n",
        "<font color=\"blue\">Used **118HHH Q1 on CeO2-NPs** for illustration, breaking into 9 components:\n",
        "* Pa = All Sentences combined in Passage\n",
        "* P1 = 1st sentence in Passage\n",
        "* P2 = 2nd sentence in Passage\n",
        "* P3 = 3rd sentence in Passage\n",
        "* Q? = the Question sentence\n",
        "* Ax = wrong choice A\n",
        "* Bv = correct answer B\n",
        "* Cx = wrong choice C\n",
        "* Dx = wrong choice D</font>"
      ],
      "metadata": {
        "id": "6NTtMzbl_5f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: CeO2 Question (219 Words)\n",
        "# This is a *** question used to directly compare with 5Vs in section 8.1 and 8.2\n",
        "sentences = [\"Pa: Some fuel additives contain cerium oxide nanoparticles (CeO2-NPs), which can leach into waterways and soils via waste water. In a 2015 study Mael Garaud and colleagues found that CeO2-NPs can accumulate in the bodies of zebra mussels (Dreissena polymorpha). While bioaccumulation of manufactured nanoparticles may be inherently worrisome, it has been hypothesized that CeO2-NPs bioaccumulation in invertebrate like D. polymorpha could serve a valuable proxy role, observing the need for manufacturers to conduct costly and intrusive sampling of vertebrate species--such as rainbow trout (Oncorhynchus mykiss), commonly used in regulatory compliance testing--for manipulative bioaccumulation, as environmental protection laws currently require.\",\n",
        "    \"P1: Some fuel additives contain cerium oxide nanoparticles (CeO2-NPs), which can leach into waterways and soils via waste water.\",\n",
        "    \"P2: In a 2015 study Mael Garaud and colleagues found that CeO2-NPs can accumulate in the bodies of zebra mussels (Dreissena polymorpha).\",\n",
        "    \"P3: While bioaccumulation of manufactured nanoparticles may be inherently worrisome, it has been hypothesized that CeO2-NPs bioaccumulation in invertebrate like D. polymorpha could serve a valuable proxy role, observing the need for manufacturers to conduct costly and intrusive sampling of vertebrate species--such as rainbow trout (Oncorhynchus mykiss), commonly used in regulatory compliance testing--for manipulative bioaccumulation, as environmental protection laws currently require.\",\n",
        "    \"Q?: Which finding, if true, would most directly weaken the hypothesis presented in the text?\",\n",
        "    \"Ax) When D. polymorpha and O. mykiss are exposed to similar levels of CeO2-NPs, concentrations of CeO2-NPs in animals of both species show little variation from individual to individual.\",\n",
        "    \"Bv) The rate of CeO2-NPs uptake in D. polymorpha differs from the rate of CeO2-NPs uptake in O. mykiss in a way that is not yet well understood by researchers.\",\n",
        "    \"Cx) D.polymorpha has been shown to accumulate several other types of manufactured nanoparticles in addition to CeO2-NPs, whereas O. mykiss has been shown to accumulate only CeO2-NPs.\",\n",
        "    \"Dx) Compared with O. mykiss, D.polymorpha can accumulate detectable CeO2-NPs concentrations with significantly fewer negative effects.\"\n",
        "]"
      ],
      "metadata": {
        "id": "xFnLqNPQ_-54"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">3.2 Examples 2-3-4-5 & Alternatives:</font>\n",
        "\n",
        "**<font color=\"blue\">SAT Questions**:\n",
        "* **Queen Egypt**: demonstrates 1) **omi_bias**.\n",
        "* **LNH Study**: demonstrates 2) **perif_bias**, 3) **long_comp_conf**, 4) **num_cont_conf**(V3), and 5) **mul-rel_conf**(V4). In the end, **num_cont_conf**(V3) and **mul_rel_conf**(V4) of 5Vs are combined to demonstrate the clear solution effect.\n",
        "* **Gabler Ulyusses** demonstrates **long_comp_conf** and 6) **ir-rel_conf**(V4).</font>\n",
        "\n",
        "**<font color=\"blue\">GRE Question**:\n",
        "* **Cotton Mather**; GRE reading question demonstrates 7) **long_pass_conf** and 8) **spec_conf** in **graduate level** texts.</font>"
      ],
      "metadata": {
        "id": "WbrBMrb36qc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">3.2.1 SAT Queen Merneith Egypt: Omi_Bias</font>"
      ],
      "metadata": {
        "id": "wFJWxIODYyRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Queen Egypt (91 words)\n",
        "# This is ** question used to discover omi_bias among choices\n",
        "sentences = [\"Pa: Archaeologist Christiana Kohler and her team excavated the Egyptian tomb of Queen Merneith, the wife of a First Dynasty pharaoh. Some scholars claim that she also ruled Egypt on her own and was actually the first female pharaoh. The team found a tablet in Merneith’s tomb with writing suggesting that she was in charge of the country’s treasury and other central offices. Whether Merneith was a pharaoh or not, this discovery supports the idea that Merneith likely _______\",\n",
        "    \"P1: Archaeologist Christiana Kohler and her team excavated the Egyptian tomb of Queen Merneith, the wife of a First Dynasty pharaoh.\",\n",
        "    \"P2: Some scholars claim that she also ruled Egypt on her own and was actually the first female pharaoh.\",\n",
        "    \"P3: The team found a tablet in Merneith’s tomb with writing suggesting that she was in charge of the country’s treasury and other central offices.\",\n",
        "    \"Q?: Whether Merneith was a pharaoh or not, this discovery supports the idea that Merneith likely _______ (choose from Av, Bx, Cx, and Dx). \",\n",
        "    \"Av) had an important role in Egypt’s government.\",\n",
        "    \"Bx) lived after rather than before the First Dynasty of Egypt.\",\n",
        "    \"Cx) traveled beyond Egypt’s borders often.\",\n",
        "    \"Dx) created a new form of writing in Egypt.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "X9rF5XJaasNx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 (Alt-1): Queen Egypt Amended  (91 words)\n",
        "# This is the revised the example 2 to solve omi_bias among choices by V2.\n",
        "sentences = [\"Pa: Archaeologist Christiana Kohler and her team excavated the Egyptian tomb of Queen Merneith, the wife of a First Dynasty pharaoh. Some scholars claim that she also ruled Egypt on her own and was actually the first female pharaoh. The team found a tablet in Merneith’s tomb with writing suggesting that she was in charge of the country’s treasury and other central offices. Whether Merneith was a pharaoh or not, this discovery supports the idea that Merneith likely _______\",\n",
        "    \"P1: Archaeologist Christiana Kohler and her team excavated the Egyptian tomb of Queen Merneith, the wife of a First Dynasty pharaoh.\",\n",
        "    \"P2: Some scholars claim that she also ruled Egypt on her own and was actually the first female pharaoh.\",\n",
        "    \"P3: The team found a tablet in Merneith’s tomb with writing suggesting that she was in charge of the country’s treasury and other central offices.\",\n",
        "    \"Q?: This discovery supports that _______ (choose from Av, Bx, Cx, and Dx)?\",\n",
        "    \"Av) Merneith likely had a role in her government\",\n",
        "    \"Bx) Merneith likely lived after the First Dynasty\",\n",
        "    \"Cx) Merneith likely traveld beyond Egypt borders\",\n",
        "    \"Dx) Merneith likely created an Egptian writing.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "T5uZUh9kj0jK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">3.2.2 SAT LNH Study: perif_bias, long_comp_conf, num_cont_conf, and mul_rel_conf</font>"
      ],
      "metadata": {
        "id": "-haN6Hj3ZI9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: LNH Study (221 Words)\n",
        "# This is a *** question used to discover perif_bias (omi_), long_comp_conf num_cont_conf, and mul_rel_conf in text.\n",
        "sentences = [\"Pa: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules. Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"P1: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules.\",\n",
        "    \"P2: Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"Pa: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules. Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’ claim?\",\n",
        "    \"Ax) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and fusion and between grammatical complexity and informativity\",\n",
        "    \"Bx) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and exotericity.\",\n",
        "    \"Cx) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical complexity and fusion.\",\n",
        "    \"Dv) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between fusion and exotericity and between informativity and exotericity.\"\n",
        "]"
      ],
      "metadata": {
        "id": "Uv8etHnIkoGz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-1): LNH Study (221 Words)\n",
        "# This is the revised Example 3 to solve perif_bias (omi_) in text by V2.\n",
        "sentences = [\"Pa: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules. Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"P1: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules.\",\n",
        "    \"P2: Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"Pa: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules. Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’ claim?\",\n",
        "    \"Ax) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and fusion and between grammatical complexity and informativity\",\n",
        "    \"Bx) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and exotericity.\",\n",
        "    \"Cx) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical complexity and fusion.\",\n",
        "    \"Dv) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical fusion and exotericity and between grammatical informativity and exotericity.\"\n",
        "]"
      ],
      "metadata": {
        "id": "vfZEp-mVA-O_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-2): LNH Study (221 Words)\n",
        "# This is the revised Example 3 to solve long_comp_conf in text (Non-V).\n",
        "sentences = [\"Pa: The Linguistic Niche Hypothesis (LNH) suggests that as more non-native speakers adopt a language, its grammatical complexity decreases. This occurs because these speakers often do not fully acquire complex grammatical rules. Olena Shcherbakova and her colleagues focused on two measures of grammatical complexity: fusion (the merging of distinct phonemes) and informativity (the capacity for meaningful variation). They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"P1: The Linguistic Niche Hypothesis (LNH) suggests that as more non-native speakers adopt a language, its grammatical complexity decreases. This occurs because these speakers often do not fully acquire complex grammatical rules.\",\n",
        "    \"P2: Olena Shcherbakova and her colleagues focused on two measures of grammatical complexity: fusion (the merging of distinct phonemes) and informativity (the capacity for meaningful variation).\",\n",
        "    \"P3: They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’claim?\",\n",
        "    \"Ax) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and fusion, as well as a slightly negative correlation between grammatical complexity and informativity.\",\n",
        "    \"Bx) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and exotericity.\",\n",
        "    \"Cx) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical complexity and fusion.\",\n",
        "    \"Dv) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical fusion and exotericity, as well as a slightly positive correlation between grammatical informativity and exotericity.\"\n",
        "]"
      ],
      "metadata": {
        "id": "fmOVbZQsSw-T"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-3): LNH Study (221 Words)\n",
        "# This is the revised Example 3 to solve num-cont confusion in text by V3.\n",
        "sentences = [\"Pa: The Linguistic Niche Hypothesis (LNH) suggests that as more non-native speakers adopt a language, its grammatical complexity decreases. This occurs because these speakers often do not fully acquire complex grammatical rules. Olena Shcherbakova and her colleagues focused on two measures of grammatical complexity: fusion (the merging of distinct phonemes) and informativity (the capacity for meaningful variation). They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"P1: The Linguistic Niche Hypothesis (LNH) suggests that as more non-native speakers adopt a language, its grammatical complexity decreases. This occurs because these speakers often do not fully acquire complex grammatical rules. Three initial factors.\",\n",
        "    \"P2: Olena Shcherbakova and her colleagues focused on two measures of grammatical complexity: fusion (the merging of distinct phonemes) and informativity (the capacity for meaningful variation). Add two additional factors, and three factors are studied.\",\n",
        "    \"P3: They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’claim?\",\n",
        "    \"Ax) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and fusion, as well as a slightly negative correlation between grammatical complexity and informativity. Three factors.\",\n",
        "    \"Bx) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and exotericity. Two factors.\",\n",
        "    \"Cx) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical complexity and fusion. Two factors.\",\n",
        "    \"Dv) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical fusion and exotericity, as well as a slightly positive correlation between grammatical informativity and exotericity. Three factors.\"\n",
        "]"
      ],
      "metadata": {
        "id": "VKfypVijrVku"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-4): LNH Study (221 Words)\n",
        "# This is the revised Example 3 to solve mul-rel confusion in text by V4.\n",
        "sentences = [\"Pa: LNH suggests that exotericity != complexity, and exotericity = non-native speakers. Olena Shcherbakova and her colleagues focused on two measures: 1) fusion = complexity, and 2) informativity = complexity. They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"P1: LNH suggests that exotericity != complexity, while exotericity = non-native speakers.\",\n",
        "    \"P2: Olena Shcherbakova and her colleagues focused on two measures: 1) fusion = complexity, and 2) informativity = complexity.\",\n",
        "    \"P3: They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’claim?\",\n",
        "    \"Ax) complexity != fusion, and complexity != informativity.\",\n",
        "    \"Bx) complexity != exotericity..\",\n",
        "    \"Cx) complexity = fusion.\",\n",
        "    \"Dv) fusion = exotericity, and informativity = exotericity.\"\n",
        "]"
      ],
      "metadata": {
        "id": "zhXAbtuk8Hs2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-5): LNH Study (221 Words)\n",
        "# This is the revised Example 3 to solve mul-rel and num-conut confusions in text by V3-4.\n",
        "sentences = [\"Pa: LNH suggests that exotericity != complexity, and exotericity = non-native speakers. Olena Shcherbakova and her colleagues focused on two measures: 1) fusion = complexity, and 2) informativity = complexity. They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"P1: LNH suggests that exotericity != complexity, while exotericity = non-native speakers. Inital three factors \",\n",
        "    \"P2: Olena Shcherbakova and her colleagues focused on two measures: 1) fusion = complexity, and 2) informativity = complexity. Three factors.\",\n",
        "    \"P3: They analyzed over 1,300 languages and found their results contradict the LNH. Five factors in total, only three factors were studied.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’claim?\",\n",
        "    \"Ax) complexity != fusion, and complexity != informativity. Three factors.\",\n",
        "    \"Bx) complexity != exotericity. Two factors.\",\n",
        "    \"Cx) complexity = fusion. Two factors.\",\n",
        "    \"Dv) fusion = exotericity, and informativity = exotericity. Three factors.\"\n",
        "]"
      ],
      "metadata": {
        "id": "LcnHZBpaq6vm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">3.2.3 SAT Gabler Ulysses: long_comp_conf and ir_rel_conf</font>"
      ],
      "metadata": {
        "id": "gVcPggiobHNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Gabler Ulysses\n",
        "# This is a hard question to discover long_comp_conf and ir_rel_conf in text.\n",
        "sentences = [\"Pa: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"P1: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent.\",\n",
        "    \"P2: One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals.\",\n",
        "    \"P3: However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"Q?: Which choice most logically complete the text?\",\n",
        "    \"Ax) negative, since those Joyce scholars with the necessary expertise to write a review of the 1984 edition would be aware that facsimile manuscripts cannot be produced with a high enough fidelity to the original to ensure that relying on them will not introduce editorial errors.\",\n",
        "    \"Bx) positive, since scholars who reviewed the 1984 edition were unaffiliated with its production and were mostly either Joyce specialists who were largely unfamiliar with editorial theories and practices or specialists in such theories and practices who were insufficiently familiar with Joyce.\",\n",
        "    \"Cx) negative, since any scholar with expertise in editorial theories of the United States and United Kingdom as well as French and German editorial theories most likely worked with Gabler on the 1984 edition and would therefore not review it.\",\n",
        "    \"Dv) positive, since Ulysses is a novel in English and the 1984 edition would therefore be more widely reviewed in United States and United Kingdom publications than in French and German publications.\"\n",
        "]"
      ],
      "metadata": {
        "id": "7r-KxQSlgwZt"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 (Alt-1): Gabler Ulysses\n",
        "# This is a hard question to discover irrel confusion in text by deleting V1.\n",
        "sentences = [\"Pa: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"P1: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent.\",\n",
        "    \"P2: One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals.\",\n",
        "    \"P3: However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"Q?: Which choice most logically complete the text?\",\n",
        "    \"Ax) _______ since those Joyce scholars with the necessary expertise to write a review of the 1984 edition would be aware that facsimile manuscripts cannot be produced with a high enough fidelity to the original to ensure that relying on them will not introduce editorial errors.\",\n",
        "    \"Bx) _______ since scholars who reviewed the 1984 edition were unaffiliated with its production and were mostly either Joyce specialists who were largely unfamiliar with editorial theories and practices or specialists in such theories and practices who were insufficiently familiar with Joyce.\",\n",
        "    \"Cx) _______ since any scholar with expertise in editorial theories of the United States and United Kingdom as well as French and German editorial theories most likely worked with Gabler on the 1984 edition and would therefore not review it.\",\n",
        "    \"Dv) _______ since Ulysses is a novel in English and the 1984 edition would therefore be more widely reviewed in United States and United Kingdom publications than in French and German publications.\"\n",
        "]"
      ],
      "metadata": {
        "id": "18Fdj1r82Z8F"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 (Alt-2): Gabler Ulysses\n",
        "# This is a hard question to mitigate long-comp confusion in text by adding 5V.\n",
        "sentences = [\"Pa: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"P1: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One; Contrast; Many; Negative.\",\n",
        "    \"P2: One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. One; Two; Contrast; Negative. Effect\",\n",
        "    \"P3: However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______. One; Negative; Many.\",\n",
        "    \"Q?: Which choice most logically complete the text?\",\n",
        "    \"Ax) _______ since those Joyce scholars with the necessary expertise to write a review of the 1984 edition would be aware that facsimile manuscripts cannot be produced with a high enough fidelity to the original to ensure that relying on them will not introduce editorial errors. Many; One; negative; Positive. Effect\",\n",
        "    \"Bx) _______ since scholars who reviewed the 1984 edition were unaffiliated with its production and were mostly either Joyce specialists who were largely unfamiliar with editorial theories and practices or specialists in such theories and practices who were insufficiently familiar with Joyce. Many; One; positive; Negative.\",\n",
        "    \"Cx) _______ since any scholar with expertise in editorial theories of the United States and United Kingdom as well as French and German editorial theories most likely worked with Gabler on the 1984 edition and would therefore not review it. Many; One; Two; Compare. negavie; Negative\",\n",
        "    \"Dv) _______ since Ulysses is a novel in English and the 1984 edition would therefore be more widely reviewed in United States and United Kingdom publications than in French and German publications. One; Two; Contrast. positive; Positive.\"\n",
        "]"
      ],
      "metadata": {
        "id": "rkjGfDgKi7A7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 (Alt-3): Gabler Ulysses\n",
        "# This is a hard question to solve long-comp confusion in text by converting to 5V.\n",
        "sentences = [\"Pa: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"P1: One; Contrast; Many; Negative.\",\n",
        "    \"P2: One; Negative Two; Contrast; Effect; Negative\",\n",
        "    \"P3: One; Negative; Many.\",\n",
        "    \"Q?: Which choice most logically match the text?\",\n",
        "    \"Ax) Many; One; negative; Positive. Effect\",\n",
        "    \"Bx) Many; One; positive; Negative.\",\n",
        "    \"Cx) Many; One; Two; Compare. negative; Negative\",\n",
        "    \"Dv) One; Two; Contrast. positive; Positive.\"\n",
        "]"
      ],
      "metadata": {
        "id": "RnQ18sirmf_v"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">3.2.4 GRE Cotton Mother: long_pass_conf and ir_rel_conf</font>"
      ],
      "metadata": {
        "id": "3zX31PXvdcE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: GRE Cotton Mother\n",
        "# This is a hard question GRE reading used to discover long_pass_conf and spec_conf in graduate level text.\n",
        "sentences = [\"Pa: Among many historians a belief persists that Cotton Mather's biographies of some of the settlers of the Massachusetts Bay Colony (published 1702) are exercises in hagiography, endowing their subjects with saintly piety at the expense of historical accuracy. Yet modern studies have profited both from the breadth of information that Mather provides in, for example, his discussions of colonial medicine and from his critical observations of such leading figures as Governor John Winthrop. Mather's wry humor as demonstrated by his detailed descriptions of events such as Winthrop's efforts to prevent wood-stealing is overlooked by those charging Mather with presenting his subjects as extremely pious. The charge also obscures Mather's concern with the settlers material, not just spiritual, prosperity. Further, this pejorative view underrates the biographies value as chronicles: Mather amassed all sorts of published and unpublished documents as sources, and his selection of key eventsshows a marked sensitivity to the nature of the colony's development.\",\n",
        "    \"P1: Among many historians a belief persists that Cotton Mather's biographies of some of the settlers of the Massachusetts Bay Colony (published 1702) are exercises in hagiography, endowing their subjects with saintly piety at the expense of historical accuracy.\",\n",
        "    \"P2: Yet modern studies have profited both from the breadth of information that Mather provides in, for example, his discussions of colonial medicine and from his critical observations of such leading figures as Governor John Winthrop.\",\n",
        "    \"P3: Mather's wry humor as demonstrated by his detailed descriptions of events such as Winthrop's efforts to prevent wood-stealing is overlooked by those charging Mather with presenting his subjects as extremely pious.\",\n",
        "    \"P4: The charge also obscures Mather's concern with the settlers material, not just spiritual, prosperity. (P5:) Further, this pejorative view underrates the biographies value as chronicles: Mather amassed all sorts of published and unpublished documents as sources, and his selection of key eventsshows a marked sensitivity to the nature of the colony's development.\",\n",
        "    \"Ax) The primary purpose of the passage is to argue against a theory universally accepted by historical researchers\",\n",
        "    \"Bx) The primary purpose of the passage is to call attention to an unusual approach to documenting a historical era\",\n",
        "    \"Cx) The primary purpose of the passage is to summarize research on a specific historical figure\",\n",
        "    \"Dv) The primary purpose of the passage is to counter a particular view about the work of a biographer\",\n",
        "    \"Ex) The primary purpose of the passage is to point out subtle differences among controversial historical reports\"\n",
        "]"
      ],
      "metadata": {
        "id": "tRBeMYKlMRbt"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 (Alt-1): GRE Cotton Mother\n",
        "# This is the revised Example 3 to mitigate long_pass_conf in graduate level text.\n",
        "sentences = [\"Pa: Among many historians a belief persists that Cotton Mather's biographies of some of the settlers of the Massachusetts Bay Colony (published 1702) are exercises in hagiography, endowing their subjects with saintly piety at the expense of historical accuracy. Yet modern studies have profited both from the breadth of information that Mather provides in, for example, his discussions of colonial medicine and from his critical observations of such leading figures as Governor John Winthrop. Mather's wry humor as demonstrated by his detailed descriptions of events such as Winthrop's efforts to prevent wood-stealing is overlooked by those charging Mather with presenting his subjects as extremely pious. The charge also obscures Mather's concern with the settlers material, not just spiritual, prosperity. Further, this pejorative view underrates the biographies value as chronicles: Mather amassed all sorts of published and unpublished documents as sources, and his selection of key eventsshows a marked sensitivity to the nature of the colony's development.\",\n",
        "    \"P1: Many believed that Cotton Mather's biographies of settlers of Massachusetts Bay Colony are exercises in hagiography.\",\n",
        "    \"P2: Yet modern studies have profited both from information that Mather provides and from his critical observations of leading figures.\",\n",
        "    \"P3: Mather's wry humor is overlooked by those charging Mather with presenting his subjects as extremely pious.\",\n",
        "    \"P4: The charge also obscures Mather's concern with the settlers material. (P5:) Further, this pejorative view underrates the biographies value as chronicles.\",\n",
        "    \"Ax) The primary purpose of the passage is to argue against a theory universally accepted by historical researchers\",\n",
        "    \"Bx) The primary purpose of the passage is to call attention to an unusual approach to documenting a historical era\",\n",
        "    \"Cx) The primary purpose of the passage is to summarize research on a specific historical figure\",\n",
        "    \"Dv) The primary purpose of the passage is to counter a particular view about the work of a biographer\",\n",
        "    \"Ex) The primary purpose of the passage is to point out subtle differences among controversial historical reports\"\n",
        "]"
      ],
      "metadata": {
        "id": "Vio4gQQvT8BM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 (Alt-2): GRE Cotton Mother\n",
        "# This is the revised Example 3 to mitigate spec_conf in text by adding 5V.\n",
        "sentences = [\"Pa: Among many historians a belief persists that Cotton Mather's biographies of some of the settlers of the Massachusetts Bay Colony (published 1702) are exercises in hagiography, endowing their subjects with saintly piety at the expense of historical accuracy. Yet modern studies have profited both from the breadth of information that Mather provides in, for example, his discussions of colonial medicine and from his critical observations of such leading figures as Governor John Winthrop. Mather's wry humor as demonstrated by his detailed descriptions of events such as Winthrop's efforts to prevent wood-stealing is overlooked by those charging Mather with presenting his subjects as extremely pious. The charge also obscures Mather's concern with the settlers material, not just spiritual, prosperity. Further, this pejorative view underrates the biographies value as chronicles: Mather amassed all sorts of published and unpublished documents as sources, and his selection of key eventsshows a marked sensitivity to the nature of the colony's development.\",\n",
        "    \"P1: Many believed that Cotton Mather's biographies of settlers of Massachusetts Bay Colony are exercises in hagiography. One; Subjective.\",\n",
        "    \"P2: Yet modern studies have profited both from information that Mather provides and from his critical observations of leading figures. Two; Objective. Positive.\",\n",
        "    \"P3: Mather's wry humor is overlooked by those charging Mather with presenting his subjects as extremely pious. One; Objective; Negative.\",\n",
        "    \"P4: The charge also obscures Mather's concern with the settlers material. One; Negative; Subjective. (P5:) Further, this pejorative view underrates the biographies value as chronicles. One; Subjective; Negative.\",\n",
        "    \"Ax) The primary purpose of the passage is to argue against a theory universally accepted by historical researchers. One; Many; Objective.\",\n",
        "    \"Bx) The primary purpose of the passage is to call attention to an unusual approach to documenting a historical era. One; Many; Subjective.\",\n",
        "    \"Cx) The primary purpose of the passage is to summarize research on a specific historical figure. One; Subjective.\",\n",
        "    \"Dv) The primary purpose of the passage is to counter a particular view about the work of a biographer. One; Subjective; Negative\",\n",
        "    \"Ex) The primary purpose of the passage is to point out subtle differences among controversial historical reports. Two; Negative; Objective; Contrast.\"\n",
        "]"
      ],
      "metadata": {
        "id": "kNdkUoJgVpNY"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 (Alt-3): GRE Cotton Mother\n",
        "# This is the revised Example 3 to solve spec_conf in text by converting to 5V.\n",
        "sentences = [\"Pa: One; Subjective; Negative.\",\n",
        "    \"P1: Many: One; Subjective.\",\n",
        "    \"P2: Two; Objective. Positive.\",\n",
        "    \"P3: One; Objective; Negative.\",\n",
        "    \"P4: One; Negative; Subjective. One; Subjective; Negative.\",\n",
        "    \"Ax) One; Many; Objective.\",\n",
        "    \"Bx) One; Many; Subjective.\",\n",
        "    \"Cx) One; Subjective.\",\n",
        "    \"Dv) One; Subjective; Negative\",\n",
        "    \"Ex) Two; Negative; Objective; Contrast.\"\n",
        "]"
      ],
      "metadata": {
        "id": "AM49t10yx-l9"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"blue\">4. Generate Embeddings for CeO2 Sample Sentences.</font>\n",
        "\n",
        "<font color=\"blue\">Used a pre-trained Sentence Transformer model, `all-mpnet-base-v2`, to generate embeddings for each sentence. This model is a good general-purpose choice, with mapping of sentences & paragraphs to a **768** dimensional dense vector space and can be used for tasks like clustering or semantic search.</font>\n",
        "\n",
        "<font color=\"blue\">`all-mpnet-base-v2`: is a sentence transformer model that converts sentences and paragraphs into numerical vectors. These vectors, called embeddings, capture the semantic meaning of the text.</font>\n",
        "\n",
        "**<font color=\"blue\">Key Features and What it's Used For**:\n",
        "* Sentence Embeddings: The model takes a sentence (or short paragraph) as input and produces a dense vector representation.\n",
        "* 768 Dimensions: is a common dimensionality for these types of models.\n",
        "* MPNet Architecture: is a model that combines the strengths of BERT and XLNet to better understand word order and context.\n",
        "* General Purpose: `all-mpnet-base-v2` is designed to be a general-purpose model, meaning it performs well on a variety of tasks.</font>\n",
        "\n",
        "**<font color=\"blue\">Common Applications**:\n",
        "* Semantic Search: Finding sentences or documents with similar meanings.\n",
        "* Information Retrieval: Pulling up relevant information based on a text query.\n",
        "* Clustering: Grouping sentences or paragraphs with similar meanings.\n",
        "* Sentence Similarity: Measuring how alike two pieces of text are.</font>\n",
        "\n",
        "**<font color=\"blue\">How to Use It** (in Python with Sentence Transformers):</font>\n",
        "\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model = SentenceTransformer('all-mpnet-base-v2')\n",
        "        sentences = [\n",
        "        \"This is a simple example.\",\n",
        "        \"Here is another sentence.\",\n",
        "        \"A third sentence for demonstration.\"\n",
        "    ]\n",
        "    embeddings = model.encode(sentences)\n",
        "    print(embeddings)  # Output: A list of 768-dimensional vectors\n",
        "\n",
        "**<font color=\"blue\">Why is it Popular?**\n",
        "* Inclusive dataset: a massive dataset of over 1 billion sentence pairs from\n",
        "Natural Language Inference (NLI) datasets, Paraphrase datasets, and a large collection of other published English data from various sources.\n",
        "* Good Performance: It generally ranks high in accuracy for many semantic text understanding tasks.\n",
        "* Efficiency: While very effective, it's also relatively efficient to use.\n",
        "* Ease of Use: Libraries like Sentence Transformers make it very easy to download and use.</font>\n",
        "\n",
        "**<font color=\"blue\">So, Let run the embedding code below** ↓ ↓ ↓ embedding ↓ ↓ ↓</font>"
      ],
      "metadata": {
        "id": "mFQ_kUZAA5RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes ~30sec in first run within a runtime because of model loading.\n",
        "# Additional sun takes 5-10sec.\n",
        "# Load the pre-trained model\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Generate the embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "print(\"Shape of embeddings:\", embeddings.shape)\n",
        "print(\"Example embedding (last sentence):\\n\", embeddings[8][:100]) # Print the first 100 dimensions only"
      ],
      "metadata": {
        "id": "O_TbU_wZBkSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"blue\">5. Reduce Dimensionality: 2D with PCA</font>\n",
        "\n",
        "<font color=\"blue\">**PCA** = principal component analysis.</font>\n",
        "\n",
        "<font color=\"blue\">Principal Components are composite dimensions from the existing 768-dimention embedding dataset. It helps to reduce dimensions while maintaning the most variances mathematically, thus being able to discern the most of datapoints. The ability to discern things is simply the intelligence, or smartness.</font>"
      ],
      "metadata": {
        "id": "ocb8gsESB1VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumed Embeddings from section 4.Generate Embeddings for Sentences.\n",
        "# Reduce to 2 dimensions using PCA\n",
        "pca_2d = PCA(n_components=2)\n",
        "reduced_embeddings_2d = pca_2d.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the 2D embeddings\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.scatter(reduced_embeddings_2d[:, 0], reduced_embeddings_2d[:, 1])\n",
        "\n",
        "# Annotate each point with the corresponding sentence\n",
        "for i, txt in enumerate(sentences):\n",
        "    plt.annotate(txt[:2], (reduced_embeddings_2d[i, 0], reduced_embeddings_2d[i, 1]))\n",
        "\n",
        "plt.title(\"2D Visualization of Sentence Embeddings (PCA)\")\n",
        "plt.xlabel(\"PC 1\")\n",
        "plt.ylabel(\"PC 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fJeP42tQCDmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"blue\">6. Reduce Dimensionality: 3D with PCA</font>\n",
        "\n",
        "### <font color=\"blue\">6.1 3D PCA static</font>\n"
      ],
      "metadata": {
        "id": "uc8WbutRCRBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumed Embeddings from section 4.Generate Embeddings for Sentences.\n",
        "# Reduce to 3D scatterplot using PCA\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the 3D embeddings\n",
        "fig_3d = plt.figure(figsize=(8, 6))\n",
        "ax_3d = fig_3d.add_subplot(111, projection='3d')\n",
        "scatter_3d = ax_3d.scatter(reduced_embeddings_3d[:, 0], reduced_embeddings_3d[:, 1], reduced_embeddings_3d[:, 2])\n",
        "\n",
        "# Annotate each point with the corresponding sentence\n",
        "for i, txt in enumerate(sentences):\n",
        "    ax_3d.text(reduced_embeddings_3d[i, 0], reduced_embeddings_3d[i, 1], reduced_embeddings_3d[i, 2], txt[:2])\n",
        "\n",
        "ax_3d.set_xlabel(\"PC 1\")\n",
        "ax_3d.set_ylabel(\"PC 2\")\n",
        "ax_3d.set_zlabel(\"PC 3\")\n",
        "ax_3d.set_title(\"3D Visualization of Sentence Embeddings (PCA)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fySiUjFOCa9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">6.2 3D PCA rotational with P-tet</font>\n",
        "\n",
        "<font color=\"blue\">Question domain is illustrated in P-tet. Choices relation to Prompt-Question is shown by the distance from Choice to P-tet.</font>"
      ],
      "metadata": {
        "id": "dnOCVriiwN5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D PCA Scatterplo and P-Tet\n",
        "# Assumed Embeddings from section 4.Generate Embeddings for Sentences.\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "labels = [s[:2] for s in sentences]  # Get first two letters.\n",
        "\n",
        "# Perform PCA to reduce to 3 dimensions\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "# Directly define the indices, assuming P1, P2, P3, Q? are at indices 1, 2, 3, and 4\n",
        "p1_index = 1\n",
        "p2_index = 2\n",
        "p3_index = 3\n",
        "q_index = 4\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_embeddings_3d[p1_index]\n",
        "p2_coords = reduced_embeddings_3d[p2_index]\n",
        "p3_coords = reduced_embeddings_3d[p3_index]\n",
        "q_coords = reduced_embeddings_3d[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}'\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Create the 3D scatter plot and add the lines\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=reduced_embeddings_3d[:, 0],\n",
        "    y=reduced_embeddings_3d[:, 1],\n",
        "    z=reduced_embeddings_3d[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,  # Use first 2 characters of labels\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=6),\n",
        "    name='Data Points'\n",
        ")] + lines)  # Combine scatter and lines\n",
        "\n",
        "# Set the title and axis labels\n",
        "fig.update_layout(\n",
        "    title=\"3D Scatterplot and P-Tet\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hEAM8I15WJ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">6.3 P-sphere, P-tet, and Scatterplot</font>\n",
        "\n",
        "<font color=\"blue\">This is the primary demonstration of analytical result.</font>"
      ],
      "metadata": {
        "id": "ZeBZ5VgiqmWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes 1-3sec in any run.\n",
        "# Assumed Embeddings from section 4.Generate Embeddings for Sentences.\n",
        "# Sphere, tetrahedron, and scatterplot.\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "labels = [s[:2] for s in sentences]  # Get first two letters.\n",
        "\n",
        "# Perform PCA to reduce to 3 dimensions\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "# Directly define the indices, assuming P1, P2, P3, Q? are at indices 1, 2, 3, and 4\n",
        "p1_index = 1\n",
        "p2_index = 2\n",
        "p3_index = 3\n",
        "q_index = 4\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_embeddings_3d[p1_index]\n",
        "p2_coords = reduced_embeddings_3d[p2_index]\n",
        "p3_coords = reduced_embeddings_3d[p3_index]\n",
        "q_coords = reduced_embeddings_3d[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}'\n",
        "        )\n",
        "    )\n",
        "\n",
        "# --- Start of new code for sphere calculation ---\n",
        "# Points for sphere calculation\n",
        "P = [p1_coords, p2_coords, p3_coords, q_coords]\n",
        "x = [p[0] for p in P]\n",
        "y = [p[1] for p in P]\n",
        "z = [p[2] for p in P]\n",
        "\n",
        "# System of equations to find sphere center (cx, cy, cz)\n",
        "# (xi-cx)^2 + (yi-cy)^2 + (zi-cz)^2 = R^2\n",
        "# Subtracting equation for P1 from P2, P3, P4:\n",
        "# 2(x2-x1)cx + 2(y2-y1)cy + 2(z2-z1)cz = (x2^2+y2^2+z2^2) - (x1^2+y1^2+z1^2)\n",
        "# ... and so on for P3 and P4\n",
        "\n",
        "A = np.array([\n",
        "    [2*(x[1]-x[0]), 2*(y[1]-y[0]), 2*(z[1]-z[0])],\n",
        "    [2*(x[2]-x[0]), 2*(y[2]-y[0]), 2*(z[2]-z[0])],\n",
        "    [2*(x[3]-x[0]), 2*(y[3]-y[0]), 2*(z[3]-z[0])]\n",
        "])\n",
        "\n",
        "B = np.array([\n",
        "    (x[1]**2 + y[1]**2 + z[1]**2) - (x[0]**2 + y[0]**2 + z[0]**2),\n",
        "    (x[2]**2 + y[2]**2 + z[2]**2) - (x[0]**2 + y[0]**2 + z[0]**2),\n",
        "    (x[3]**2 + y[3]**2 + z[3]**2) - (x[0]**2 + y[0]**2 + z[0]**2)\n",
        "])\n",
        "\n",
        "try:\n",
        "    sphere_center = np.linalg.solve(A, B)\n",
        "    cx, cy, cz = sphere_center[0], sphere_center[1], sphere_center[2]\n",
        "    sphere_radius = np.sqrt((x[0]-cx)**2 + (y[0]-cy)**2 + (z[0]-cz)**2)\n",
        "\n",
        "    # Generate sphere surface points\n",
        "    u = np.linspace(0, 2 * np.pi, 50) # Azimuthal angle\n",
        "    v = np.linspace(0, np.pi, 25)    # Polar angle\n",
        "\n",
        "    sphere_x = cx + sphere_radius * np.outer(np.cos(u), np.sin(v))\n",
        "    sphere_y = cy + sphere_radius * np.outer(np.sin(u), np.sin(v))\n",
        "    sphere_z = cz + sphere_radius * np.outer(np.ones(np.size(u)), np.cos(v)) # Corrected from np.ones_like(u) to np.ones(np.size(u))\n",
        "\n",
        "    sphere_surface = go.Surface(\n",
        "        x=sphere_x, y=sphere_y, z=sphere_z,\n",
        "        opacity=0.3,\n",
        "        colorscale='Blues', # You can choose other colorscales e.g. 'Viridis', 'RdBu'\n",
        "        showscale=False, # Hide the color scale bar for the sphere\n",
        "        name='Sphere'\n",
        "    )\n",
        "    sphere_added = True\n",
        "except np.linalg.LinAlgError:\n",
        "    print(\"Could not determine the sphere: Points might be coplanar or collinear.\")\n",
        "    sphere_surface = None # No sphere to add if calculation fails\n",
        "    sphere_added = False\n",
        "# --- End of new code for sphere calculation ---\n",
        "\n",
        "# Create the 3D scatter plot and add the lines and sphere\n",
        "data_elements = [go.Scatter3d(\n",
        "    x=reduced_embeddings_3d[:, 0],\n",
        "    y=reduced_embeddings_3d[:, 1],\n",
        "    z=reduced_embeddings_3d[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,  # Use first 2 characters of labels\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4),\n",
        "    name='Data Points'\n",
        ")] + lines\n",
        "\n",
        "if sphere_added and sphere_surface:\n",
        "    data_elements.append(sphere_surface)\n",
        "\n",
        "fig = go.Figure(data=data_elements)\n",
        "\n",
        "\n",
        "# Set the title and axis labels\n",
        "fig.update_layout(\n",
        "    title=\"3D Scatterplot, P-Tet and Sphere\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "        # Aspect ratio to make the sphere look more like a sphere\n",
        "        aspectmode='data' # 'auto', 'cube', 'data', 'manual'\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6OJW4SC5ARB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">6.4 3D PCA from multiple angles static</font>\n",
        "\n",
        "<font color=\"blue\">In case rotational view above isn't available at temp environment, use multi-angle to illustrate.</font>"
      ],
      "metadata": {
        "id": "iGJWUAOfxByM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumed Embeddings from section 4.Generate Embeddings for Sentences.\n",
        "# PCA from multiple angles static views\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "#Create various angles for multiple 3-D rendering\n",
        "elevations = [30, 30, 0, -30]  # Angles of elevation\n",
        "azim_angles = [0, 45, 90, 135] # Azimuthal angles\n",
        "\n",
        "for i, (elev, azim) in enumerate(zip(elevations, azim_angles)):\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(reduced_embeddings_3d[:, 0], reduced_embeddings_3d[:, 1], reduced_embeddings_3d[:, 2])\n",
        "    for j, txt in enumerate(sentences):\n",
        "        ax.text(reduced_embeddings_3d[j, 0], reduced_embeddings_3d[j, 1], reduced_embeddings_3d[j, 2], txt[:2])\n",
        "    ax.set_xlabel(\"PC 1\")\n",
        "    ax.set_ylabel(\"PC 2\")\n",
        "    ax.set_zlabel(\"PC 3\")\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    plt.title(f\"3D Plot (Elev={elev}, Azim={azim})\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qzJJQ2qd59hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"blue\">7. Using t-SNE for Dimensionality Reduction</font>\n",
        "\n",
        "<font color=\"blue\">**t-SNE** = t-distributed stochastic neighbor embedding. Unlike **PCA**, **t-SNE** demonstrate the embeddings in non-linear more scattered fashion.</font>\n"
      ],
      "metadata": {
        "id": "E2N5RVTwCs8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">7.1 2D t-SNE</font>"
      ],
      "metadata": {
        "id": "ZKd0J7HaxlFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D Scatterplot t-SNE\n",
        "# Assumed Embeddings from section 4.Generate Embeddings for Sentences.\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce to 2 dimensions using t-SNE with a lower perplexity\n",
        "tsne_2d = TSNE(n_components=2, random_state=42, n_iter=300, perplexity=min(5, len(sentences) - 1)) # Set perplexity <= 5 or n_samples - 1\n",
        "reduced_embeddings_tsne_2d = tsne_2d.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the 2D embeddings (t-SNE)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(reduced_embeddings_tsne_2d[:, 0], reduced_embeddings_tsne_2d[:, 1])\n",
        "\n",
        "# Annotate each point with the corresponding sentence\n",
        "for i, txt in enumerate(sentences):\n",
        "    plt.annotate(txt[:2], (reduced_embeddings_tsne_2d[i, 0], reduced_embeddings_tsne_2d[i, 1]))\n",
        "\n",
        "plt.title(\"2D Scatterplot t-SNE\")\n",
        "plt.xlabel(\"t-SNE D1\")\n",
        "plt.ylabel(\"t-SNE D2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8lgTh7bOEX6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">7.2 3D t-SNE static</font>"
      ],
      "metadata": {
        "id": "NgbblG8Mxsy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D Scatterplot t-SNE\n",
        "# 2D Scatterplot t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Reduce to 3 dimensions using t-SNE with a lower perplexity\n",
        "tsne_3d = TSNE(n_components=3, random_state=42, n_iter=300, perplexity=min(5, len(sentences) - 1))\n",
        "reduced_embeddings_tsne_3d = tsne_3d.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the 3D embeddings\n",
        "fig_3d = plt.figure(figsize=(4, 3))\n",
        "ax_3d = fig_3d.add_subplot(111, projection='3d')\n",
        "scatter_3d = ax_3d.scatter(reduced_embeddings_tsne_3d[:, 0], reduced_embeddings_tsne_3d[:, 1], reduced_embeddings_tsne_3d[:, 2])\n",
        "\n",
        "# Annotate each point with the first letter of the sentence\n",
        "for i, txt in enumerate(sentences):\n",
        "    ax_3d.text(reduced_embeddings_tsne_3d[i, 0], reduced_embeddings_tsne_3d[i, 1], reduced_embeddings_tsne_3d[i, 2], txt[:2])\n",
        "\n",
        "ax_3d.set_xlabel(\"t-SNE D1\")\n",
        "ax_3d.set_ylabel(\"t-SNE D2\")\n",
        "ax_3d.set_zlabel(\"t-SNE D3\")\n",
        "ax_3d.set_title(\" 3D Scatterplot t-SNE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VoIIlz1E3e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">7.3 3D t-SNE rotational</font>"
      ],
      "metadata": {
        "id": "UinugAfyxyO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D Scatterplot and Tetrehedron t-SNE rotational\n",
        "# Assumed embeddings from Section 4. General Embeddings for Sentences\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE # Import TSNE\n",
        "# from sklearn.decomposition import PCA # PCA is no longer needed for reduction\n",
        "\n",
        "labels = [s[:2] for s in sentences]  # Get first two letters.\n",
        "\n",
        "# Perform t-SNE to reduce to 3 dimensions\n",
        "# Note: For t-SNE, perplexity should be less than the number of samples.\n",
        "# For 9 samples, a perplexity like 2-5 is suitable.\n",
        "# learning_rate='auto' and init='pca' are often good starting points.\n",
        "# random_state is for reproducibility.\n",
        "tsne_3d = TSNE(n_components=3, perplexity=4, learning_rate='auto', init='pca', n_iter=500, random_state=42)\n",
        "reduced_embeddings_tsne_3d = tsne_3d.fit_transform(embeddings)\n",
        "\n",
        "# Directly define the indices, assuming P1, P2, P3, Q? are at indices 1, 2, 3, and 4\n",
        "# (This depends on the order in your 'sentences' and 'embeddings' data)\n",
        "p1_index = 1 # Corresponds to \"P1\" if sentences[1] is \"P1: ...\"\n",
        "p2_index = 2 # Corresponds to \"P2\"\n",
        "p3_index = 3 # Corresponds to \"P3\"\n",
        "q_index = 4 # Corresponds to \"Q?\"\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q? from t-SNE results\n",
        "p1_coords = reduced_embeddings_tsne_3d[p1_index]\n",
        "p2_coords = reduced_embeddings_tsne_3d[p2_index]\n",
        "p3_coords = reduced_embeddings_tsne_3d[p3_index]\n",
        "q_coords = reduced_embeddings_tsne_3d[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices for the tetrahedron_vertices array)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from the first vertex (p1_coords) to others\n",
        "    (1, 2), (1, 3),          # Edges from the second vertex (p2_coords)\n",
        "    (2, 3)                   # Edge from the third vertex (p3_coords)\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_idx_in_tetra_array, end_idx_in_tetra_array) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_idx_in_tetra_array]\n",
        "    end_point = tetrahedron_vertices[end_idx_in_tetra_array]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}'\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Create the 3D scatter plot and add the lines\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=reduced_embeddings_tsne_3d[:, 0],\n",
        "    y=reduced_embeddings_tsne_3d[:, 1],\n",
        "    z=reduced_embeddings_tsne_3d[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,  # Use first 2 characters of labels\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=8),\n",
        "    name='Data Points'\n",
        ")] + lines)  # Combine scatter and lines\n",
        "\n",
        "# Set the title and axis labels for t-SNE\n",
        "fig.update_layout(\n",
        "    title=\"3D Scatterplot t-SNE rotational\", # Updated title\n",
        "    scene=dict(\n",
        "        xaxis_title=\"t-SNE D1\", # Updated x-axis label\n",
        "        yaxis_title=\"t-SNE D2\", # Updated y-axis label\n",
        "        zaxis_title=\"t-SNE D3\", # Updated z-axis label\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6KsiQwZsNz7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">7.4 3D t-SNE P-tet rotational</font>"
      ],
      "metadata": {
        "id": "mEyZgvkwSAtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D Scatterplot and tetrehedron t-SNE rotational\n",
        "# Assumed embeddings from Section 4. General Embeddings for Sentences\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "labels = [s[:2] for s in sentences] # get the first 2 letters for label scatterplot\n",
        "\n",
        "# Function to find the sphere passing through 4 points\n",
        "def get_sphere_coeffs(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the coefficients G, H, I, K for the sphere equation:\n",
        "    x^2 + y^2 + z^2 + Gx + Hy + Iz + K = 0\n",
        "    passing through four points p1, p2, p3, p4.\n",
        "    Returns (G, H, I, K) or None if points are coplanar/collinear (matrix is singular).\n",
        "    \"\"\"\n",
        "    points = np.array([p1, p2, p3, p4])\n",
        "    A = np.zeros((4, 4))\n",
        "    B = np.zeros(4)\n",
        "\n",
        "    for i in range(4):\n",
        "        A[i, 0] = points[i, 0]  # x\n",
        "        A[i, 1] = points[i, 1]  # y\n",
        "        A[i, 2] = points[i, 2]  # z\n",
        "        A[i, 3] = 1             # K's coefficient\n",
        "        B[i] = -(points[i, 0]**2 + points[i, 1]**2 + points[i, 2]**2) # -(x^2 + y^2 + z^2)\n",
        "\n",
        "    try:\n",
        "        coeffs = np.linalg.solve(A, B)\n",
        "        return coeffs # G, H, I, K\n",
        "    except np.linalg.LinAlgError:\n",
        "        print(\"Points might be coplanar or collinear. Cannot uniquely define a sphere.\")\n",
        "        return None\n",
        "\n",
        "def get_sphere_center_radius(G, H, I, K):\n",
        "    \"\"\"\n",
        "    Calculates the center (cx, cy, cz) and radius R of a sphere\n",
        "    given coefficients G, H, I, K from the equation:\n",
        "    x^2 + y^2 + z^2 + Gx + Hy + Iz + K = 0\n",
        "    \"\"\"\n",
        "    cx = -G / 2\n",
        "    cy = -H / 2\n",
        "    cz = -I / 2\n",
        "    R_squared = (G**2 + H**2 + I**2) / 4 - K\n",
        "    if R_squared < 0:\n",
        "        print(f\"Warning: R_squared is negative ({R_squared}). Something is wrong with the sphere calculation.\")\n",
        "        return (cx, cy, cz), 0 # Or handle error appropriately\n",
        "    R = np.sqrt(R_squared)\n",
        "    return (cx, cy, cz), R\n",
        "\n",
        "# Perform t-SNE to reduce to 3 dimensions\n",
        "# Adjust perplexity: must be less than n_samples\n",
        "n_samples = embeddings.shape[0]\n",
        "perplexity_value = min(30, n_samples - 1)\n",
        "if perplexity_value <= 0 : # edge case for very few samples\n",
        "    perplexity_value = max(1, n_samples -1)\n",
        "\n",
        "\n",
        "tsne_3d = TSNE(n_components=3, random_state=42, perplexity=perplexity_value, n_iter=1000)\n",
        "reduced_embeddings_tsne_3d = tsne_3d.fit_transform(embeddings)\n",
        "\n",
        "# Directly define the indices\n",
        "p1_index = 1\n",
        "p2_index = 2\n",
        "p3_index = 3\n",
        "q_index = 4\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_embeddings_tsne_3d[p1_index]\n",
        "p2_coords = reduced_embeddings_tsne_3d[p2_index]\n",
        "p3_coords = reduced_embeddings_tsne_3d[p3_index]\n",
        "q_coords = reduced_embeddings_tsne_3d[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron (these are the points for the sphere)\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# --- Sphere Calculation ---\n",
        "sphere_coeffs = get_sphere_coeffs(p1_coords, p2_coords, p3_coords, q_coords)\n",
        "sphere_trace = None\n",
        "\n",
        "if sphere_coeffs is not None:\n",
        "    G, H, I, K_val = sphere_coeffs\n",
        "    (cx, cy, cz), R = get_sphere_center_radius(G, H, I, K_val)\n",
        "\n",
        "    if R > 0: # Proceed only if radius is valid\n",
        "        # Generate sphere surface points\n",
        "        phi = np.linspace(0, np.pi, 50)\n",
        "        theta = np.linspace(0, 2 * np.pi, 50)\n",
        "        phi, theta = np.meshgrid(phi, theta)\n",
        "\n",
        "        x_sphere = cx + R * np.sin(phi) * np.cos(theta)\n",
        "        y_sphere = cy + R * np.sin(phi) * np.sin(theta)\n",
        "        z_sphere = cz + R * np.cos(phi)\n",
        "\n",
        "        sphere_trace = go.Surface(\n",
        "            x=x_sphere, y=y_sphere, z=z_sphere,\n",
        "            colorscale='Blues', # Or any other colorscale\n",
        "            opacity=0.2,      # Make it semi-transparent\n",
        "            showscale=False,  # Hide color scale for the sphere\n",
        "            name='Sphere (P1-P2-P3-Q?)'\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Sphere radius is {R}, not plotting sphere.\")\n",
        "else:\n",
        "    print(\"Could not calculate sphere coefficients. Sphere will not be plotted.\")\n",
        "# --- End Sphere Calculation ---\n",
        "\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices relative to tetrahedron_vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),\n",
        "            name=f'Edge {i+1}'\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Create the 3D scatter plot\n",
        "plot_data = [go.Scatter3d(\n",
        "    x=reduced_embeddings_tsne_3d[:, 0],\n",
        "    y=reduced_embeddings_tsne_3d[:, 1],\n",
        "    z=reduced_embeddings_tsne_3d[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=8, color=np.arange(len(labels)), colorscale='Viridis'), # Color points for better distinction\n",
        "    name='Data Points'\n",
        ")] + lines\n",
        "\n",
        "if sphere_trace:\n",
        "    plot_data.append(sphere_trace)\n",
        "\n",
        "fig = go.Figure(data=plot_data)\n",
        "\n",
        "# Set the title and axis labels\n",
        "fig.update_layout(\n",
        "    title=\"3D t-SNE with Tetrahedron and Sphere\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"t-SNE D1\",\n",
        "        yaxis_title=\"t-SNE D2\",\n",
        "        zaxis_title=\"t-SNE D3\",\n",
        "        aspectmode='data' # Important for sphere to look like a sphere\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "WB6n5iDHR-R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"blue\">8. How about 5-Vs?</font>\n",
        "\n",
        "### <font color=\"blue\">8.1 Initialized 5V (CL 1wk + DR 1wk)： CeO2-NPs</font>\n",
        "\n",
        "<font color=\"blue\">Below is a typical preception with inital training/ While erros exist across V2/V3/V4/V5, the student can vaguely see the correct answer Bv from the wrong choices Ax, Cx, and Dx.</font>\n",
        "\n",
        "<font color=\"blue\">5Vs---Pa---P1---P2---P3---Q?-|-Ax---Bv---Cx---Dx---</font>\n",
        "\n",
        "<font color=\"blue\">V1-----0-----1-----0------0-----1---|--0-----1-----0-----1---</font>\n",
        "\n",
        "<font color=\"blue\">V2-----1-----0----0.5----1-----0---|--0----0.5----0----0---</font>\n",
        "\n",
        "<font color=\"blue\">V3-----1-----0----0.5----1-----1---|---1-----1-----1-----1---</font>\n",
        "\n",
        "<font color=\"blue\">V4-----0-----1-----0------0-----1---|---0-----1----0.5---1---</font>\n",
        "\n",
        "<font color=\"blue\">V5-----0-----1-----0----0.5----0---|---0-----0-----0-----1---</font>"
      ],
      "metadata": {
        "id": "1Bdv-muoBmlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">8.1.1 5V-ini: Scatterplot and P-Tet</font>"
      ],
      "metadata": {
        "id": "X08-6Zj6USyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot and P-Tet: 5V-ini\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,0\n",
        "P1,0,0,0,1,1\n",
        "P2,0,0.5,0.5,0,0\n",
        "P3,0,1,1,0,0.5\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0.5,1,1,0\n",
        "Cx,0,0,1,0.5,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "from io import StringIO\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# Get the indices of P1, P2, P3, and Q?\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the 3D scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0],\n",
        "    y=reduced_data[:, 1],\n",
        "    z=reduced_data[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4),\n",
        "    name='Data Points'  # Add a name for the scatter plot\n",
        ")\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}' # Add a name for each line.\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Combine scatter and lines\n",
        "data = [scatter] + lines\n",
        "\n",
        "# Set the title and axis labels\n",
        "layout = go.Layout(\n",
        "    title=\"Scatterplot and P-Tet: 5V-ini\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True #show legend\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "# Show the plot (this will display an interactive plot in Colab)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "5e3crrvNJ33O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">8.1.2 5V_ini scatterplot, P-Tet, and Sphere</font>"
      ],
      "metadata": {
        "id": "B9BF2ZuoUghp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/# Scatterplot, P-Tet, and P-sphere: 5V-ini\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,0\n",
        "P1,0,0,0,1,1\n",
        "P2,0,0.5,0.5,0,0\n",
        "P3,0,1,1,0,0.5\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0.5,1,1,0\n",
        "Cx,0,0,1,0.5,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# Get the indices of P1, P2, P3, and Q?\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron (also the points defining the sphere)\n",
        "sphere_defining_points = [p1_coords, p2_coords, p3_coords, q_coords]\n",
        "tetrahedron_vertices = np.array(sphere_defining_points)\n",
        "\n",
        "# --- Functions to calculate sphere center and radius ---\n",
        "def get_sphere_coeffs(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the coefficients A, B, C, D of the sphere equation:\n",
        "    x^2 + y^2 + z^2 + Ax + By + Cz + D = 0\n",
        "    passing through four points p1, p2, p3, p4.\n",
        "    \"\"\"\n",
        "    points = np.array([p1, p2, p3, p4])\n",
        "    # Form matrix M for the system M * [A, B, C, D]' = -[x^2+y^2+z^2]'\n",
        "    M = np.ones((4, 4))\n",
        "    M[:, 0] = points[:, 0]  # x coordinates\n",
        "    M[:, 1] = points[:, 1]  # y coordinates\n",
        "    M[:, 2] = points[:, 2]  # z coordinates\n",
        "\n",
        "    # Right hand side vector\n",
        "    rhs = -(points[:, 0]**2 + points[:, 1]**2 + points[:, 2]**2)\n",
        "\n",
        "    try:\n",
        "        # Solve for A, B, C, D\n",
        "        coeffs = np.linalg.solve(M, rhs)\n",
        "        return coeffs\n",
        "    except np.linalg.LinAlgError:\n",
        "        print(\"Error: The four points might be coplanar. Cannot determine a unique sphere.\")\n",
        "        return None\n",
        "\n",
        "def calculate_sphere_center_radius(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the center and radius of a sphere passing through four points.\n",
        "    p1, p2, p3, p4 are 3D points as NumPy arrays or lists [x, y, z].\n",
        "    Returns (center, radius) or (None, None) if points are coplanar.\n",
        "    \"\"\"\n",
        "    coeffs = get_sphere_coeffs(np.array(p1), np.array(p2), np.array(p3), np.array(p4))\n",
        "\n",
        "    if coeffs is None:\n",
        "        return None, None\n",
        "\n",
        "    A, B, C, D_coeff = coeffs\n",
        "\n",
        "    # Center (xc, yc, zc)\n",
        "    xc = -A / 2\n",
        "    yc = -B / 2\n",
        "    zc = -C / 2\n",
        "    center = np.array([xc, yc, zc])\n",
        "\n",
        "    # Radius R\n",
        "    # R^2 = xc^2 + yc^2 + zc^2 - D_coeff\n",
        "    radius_sq = xc**2 + yc**2 + zc**2 - D_coeff\n",
        "    if radius_sq < 0:\n",
        "        print(\"Error: Calculated radius squared is negative (points might be collinear or an issue with PCA reduction).\")\n",
        "        return None, None\n",
        "    radius = np.sqrt(radius_sq)\n",
        "\n",
        "    return center, radius\n",
        "\n",
        "# --- Function to generate Plotly sphere surface ---\n",
        "def get_plotly_sphere_surface(center, radius, color='rgba(0,180,255,0.3)', resolution=50, name='Circumsphere'):\n",
        "    \"\"\"\n",
        "    Generates Plotly go.Surface data for a sphere.\n",
        "    center: NumPy array or list for the sphere's center [xc, yc, zc].\n",
        "    radius: Radius of the sphere.\n",
        "    color: Color of the sphere as an rgba string (e.g., 'rgba(R,G,B,A)').\n",
        "    resolution: Number of points for theta and phi.\n",
        "    name: Legend name for the sphere.\n",
        "    \"\"\"\n",
        "    theta = np.linspace(0, 2 * np.pi, resolution)\n",
        "    phi = np.linspace(0, np.pi, resolution)\n",
        "    theta, phi = np.meshgrid(theta, phi)\n",
        "\n",
        "    x = center[0] + radius * np.cos(theta) * np.sin(phi)\n",
        "    y = center[1] + radius * np.sin(theta) * np.sin(phi)\n",
        "    z = center[2] + radius * np.cos(phi)\n",
        "\n",
        "    # To achieve a single color with opacity, we set the color directly in the colorscale\n",
        "    # and ensure opacity is handled by the color string itself or the opacity property of go.Surface\n",
        "\n",
        "    # Extract RGB from the rgba string if provided, default to a blue if format is unexpected\n",
        "    try:\n",
        "        rgb_color_part = color.split('(')[1].split(')')[0].split(',')\n",
        "        r, g, b = rgb_color_part[0], rgb_color_part[1], rgb_color_part[2]\n",
        "        plotly_color = f'rgb({r},{g},{b})'\n",
        "        opacity_val = float(rgb_color_part[3]) if len(rgb_color_part) > 3 else 0.3\n",
        "    except:\n",
        "        plotly_color = 'rgb(0,180,255)' # Default blue\n",
        "        opacity_val = 0.3\n",
        "\n",
        "    return go.Surface(\n",
        "        x=x, y=y, z=z,\n",
        "        colorscale=[[0, plotly_color], [1, plotly_color]], # Solid color\n",
        "        showscale=False,\n",
        "        opacity=opacity_val,\n",
        "        name=name,\n",
        "        hoverinfo='skip' # Optional: disable hover for the sphere surface\n",
        "    )\n",
        "\n",
        "# --- Plotting ---\n",
        "\n",
        "# Create the 3D scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0],\n",
        "    y=reduced_data[:, 1],\n",
        "    z=reduced_data[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4, color='black'), # Changed marker color for better visibility against sphere\n",
        "    name='Data Points'\n",
        ")\n",
        "\n",
        "# Define the edges of the tetrahedron\n",
        "tetrahedron_edges_indices = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1\n",
        "    (1, 2), (1, 3),          # Edges from P2\n",
        "    (2, 3)                   # Edge from P3\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_idx, end_idx) in enumerate(tetrahedron_edges_indices):\n",
        "    start_point = tetrahedron_vertices[start_idx]\n",
        "    end_point = tetrahedron_vertices[end_idx]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=3),\n",
        "            name=f'Tetrahedron Edge' # Simplified name\n",
        "        )\n",
        "    )\n",
        "# To avoid multiple \"Tetrahedron Edge\" legends, only the first one will show by default if names are identical.\n",
        "# Or, make them unique if needed, or group them. For simplicity, keep as is or assign name to only one.\n",
        "if lines:\n",
        "    lines[0].showlegend = True # Show legend for the first edge only as representative\n",
        "    for line_trace in lines[1:]:\n",
        "        line_trace.showlegend = False\n",
        "\n",
        "\n",
        "# Calculate sphere center and radius using the PCA-reduced coordinates\n",
        "sphere_center, sphere_radius = calculate_sphere_center_radius(p1_coords, p2_coords, p3_coords, q_coords)\n",
        "\n",
        "# Initialize data list for the figure\n",
        "data_traces = [scatter] + lines\n",
        "\n",
        "# Add sphere to the plot if calculation was successful\n",
        "sphere_trace = None\n",
        "if sphere_center is not None and sphere_radius is not None:\n",
        "    print(f\"Sphere Center (PCA coords): {sphere_center}\")\n",
        "    print(f\"Sphere Radius (PCA coords): {sphere_radius}\")\n",
        "    sphere_trace = get_plotly_sphere_surface(\n",
        "        sphere_center,\n",
        "        sphere_radius,\n",
        "        color='rgba(100, 180, 255, 0.3)', # Light blue, semi-transparent\n",
        "        resolution=40, # Lower resolution for faster rendering, increase for smoother sphere\n",
        "        name='Circumsphere P1-P2-P3-Q?'\n",
        "    )\n",
        "    data_traces.append(sphere_trace)\n",
        "else:\n",
        "    print(\"Could not calculate sphere parameters. Sphere will not be plotted.\")\n",
        "\n",
        "\n",
        "# Set the title and axis labels\n",
        "layout = go.Layout(\n",
        "    title=\"Scatterplot, P-Tet, and P-sphere: 5V-ini\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "        aspectmode='data' # 'data', 'cube', 'auto', 'manual'\n",
        "                         # 'data' ensures that the scaling of axes matches the data range\n",
        "                         # 'cube' makes the plot region a cube\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=data_traces, layout=layout)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "HLd_OFwiUZwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">8.1.3 5V_ini: Scatterplo, P-Tet, and Max_Cos_Vec_PCA</font>"
      ],
      "metadata": {
        "id": "aZ7Wz5kNVYbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot, P-Tet, and Max-Cos-Vec-PCA: 5V_ini\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,0\n",
        "P1,0,0,0,1,1\n",
        "P2,0,0.5,0.5,0,0\n",
        "P3,0,1,1,0,0.5\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0.5,1,1,0\n",
        "Cx,0,0,1,0.5,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# --- Coordinate Extraction ---\n",
        "pa_index = labels.index('Pa')\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "ax_index = labels.index('Ax')\n",
        "bv_index = labels.index('Bv')\n",
        "cx_index = labels.index('Cx')\n",
        "dx_index = labels.index('Dx')\n",
        "\n",
        "pa_coords = reduced_data[pa_index]\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "ax_coords = reduced_data[ax_index]\n",
        "bv_coords = reduced_data[bv_index]\n",
        "cx_coords = reduced_data[cx_index]\n",
        "dx_coords = reduced_data[dx_index]\n",
        "\n",
        "# --- Plotting Code Setup ---\n",
        "data_plot = [] # Initialize list for all plot traces\n",
        "\n",
        "# Scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0], y=reduced_data[:, 1], z=reduced_data[:, 2],\n",
        "    mode='markers+text', text=labels, textposition=\"middle right\",\n",
        "    marker=dict(size=4, color='blue'), name='Data Points'\n",
        ")\n",
        "data_plot.append(scatter)\n",
        "\n",
        "# Tetrahedron edges\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "tetrahedron_edges = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
        "tetra_lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    tetra_lines.append(go.Scatter3d(\n",
        "        x=[start_point[0], end_point[0]], y=[start_point[1], end_point[1]], z=[start_point[2], end_point[2]],\n",
        "        mode='lines', line=dict(color='red', width=3), name=f'Tetra Edge {i+1}', hoverinfo='none'\n",
        "    ))\n",
        "data_plot.extend(tetra_lines)\n",
        "\n",
        "# Initial five vectors (Pa, P1, P2, P3, Q)\n",
        "vector_points_coords_for_plot = {'Pa': pa_coords, 'P1': p1_coords, 'P2': p2_coords, 'P3': p3_coords, 'Q': q_coords}\n",
        "origin_plot = np.array([0, 0, 0])\n",
        "initial_vector_color = 'green'\n",
        "vector_line_width_plot = 4\n",
        "cone_sizeref_plot = 0.2  # Adjust as needed\n",
        "\n",
        "for label, p_coords_arr in vector_points_coords_for_plot.items():\n",
        "    p_coords_plot = np.array(p_coords_arr)\n",
        "    # Shaft\n",
        "    data_plot.append(go.Scatter3d(\n",
        "        x=[origin_plot[0], p_coords_plot[0]], y=[origin_plot[1], p_coords_plot[1]], z=[origin_plot[2], p_coords_plot[2]],\n",
        "        mode='lines', line=dict(color=initial_vector_color, width=vector_line_width_plot),\n",
        "        name=f'Vector to {label} (shaft)', hoverinfo='name'\n",
        "    ))\n",
        "    # Head\n",
        "    u_plot, v_plot, w_plot = p_coords_plot - origin_plot\n",
        "    data_plot.append(go.Cone(\n",
        "        x=[p_coords_plot[0]], y=[p_coords_plot[1]], z=[p_coords_plot[2]],\n",
        "        u=[u_plot], v=[v_plot], w=[w_plot],\n",
        "        sizemode=\"absolute\", sizeref=cone_sizeref_plot, showscale=False,\n",
        "        colorscale=[[0, initial_vector_color], [1, initial_vector_color]], anchor=\"tip\",\n",
        "        name=f'Vector to {label} (head)', hoverinfo='name'\n",
        "    ))\n",
        "\n",
        "# --- Cosine Similarity Calculation ---\n",
        "print(\"\\n--- Cosine Similarity Calculation ---\")\n",
        "vec_ax = ax_coords.reshape(1, -1)\n",
        "vec_bv = bv_coords.reshape(1, -1)\n",
        "vec_cx = cx_coords.reshape(1, -1)\n",
        "vec_dx = dx_coords.reshape(1, -1)\n",
        "\n",
        "# Store coordinates for Ax, Bv, Cx, Dx for easy lookup later\n",
        "coords_map = {\n",
        "    \"Ax\": ax_coords,\n",
        "    \"Bv\": bv_coords,\n",
        "    \"Cx\": cx_coords,\n",
        "    \"Dx\": dx_coords\n",
        "}\n",
        "vectors_to_check_for_sim = {\n",
        "    \"Ax\": vec_ax,\n",
        "    \"Bv\": vec_bv,\n",
        "    \"Cx\": vec_cx,\n",
        "    \"Dx\": vec_dx\n",
        "}\n",
        "\n",
        "target_vector_label = \"Pa\"\n",
        "target_vector_coords_for_sim = pa_coords.reshape(1, -1)\n",
        "\n",
        "print(f\"Calculating cosine similarity with respect to vector: {target_vector_label} (coordinates: {target_vector_coords_for_sim.flatten()})\")\n",
        "\n",
        "max_similarity = -2\n",
        "vector_label_with_max_similarity = None\n",
        "\n",
        "for label, vec in vectors_to_check_for_sim.items():\n",
        "    similarity = cosine_similarity(vec, target_vector_coords_for_sim)[0][0]\n",
        "    print(f\"Cosine similarity between {label} and {target_vector_label}: {similarity:.4f}\")\n",
        "    if similarity > max_similarity:\n",
        "        max_similarity = similarity\n",
        "        vector_label_with_max_similarity = label\n",
        "\n",
        "if vector_label_with_max_similarity:\n",
        "    print(f\"\\nThe vector from (Ax, Bv, Cx, Dx) with the maximal cosine similarity to {target_vector_label} is: {vector_label_with_max_similarity}\")\n",
        "    print(f\"Maximal cosine similarity: {max_similarity:.4f}\")\n",
        "\n",
        "    # --- Plot the vector with maximal cosine similarity ---\n",
        "    max_sim_coords = coords_map[vector_label_with_max_similarity]\n",
        "    max_sim_vector_color = 'magenta' # Distinct color for this vector\n",
        "\n",
        "    print(f\"Plotting maximal similarity vector: {vector_label_with_max_similarity} with color {max_sim_vector_color}\")\n",
        "\n",
        "    # Shaft for max similarity vector\n",
        "    data_plot.append(go.Scatter3d(\n",
        "        x=[origin_plot[0], max_sim_coords[0]],\n",
        "        y=[origin_plot[1], max_sim_coords[1]],\n",
        "        z=[origin_plot[2], max_sim_coords[2]],\n",
        "        mode='lines',\n",
        "        line=dict(color=max_sim_vector_color, width=vector_line_width_plot + 1), # Slightly thicker\n",
        "        name=f'Max Sim. to Pa: {vector_label_with_max_similarity} (shaft)',\n",
        "        hoverinfo='name'\n",
        "    ))\n",
        "    # Head for max similarity vector\n",
        "    u_max_sim, v_max_sim, w_max_sim = max_sim_coords - origin_plot\n",
        "    data_plot.append(go.Cone(\n",
        "        x=[max_sim_coords[0]], y=[max_sim_coords[1]], z=[max_sim_coords[2]],\n",
        "        u=[u_max_sim], v=[v_max_sim], w=[w_max_sim],\n",
        "        sizemode=\"absolute\", sizeref=cone_sizeref_plot, showscale=False,\n",
        "        colorscale=[[0, max_sim_vector_color], [1, max_sim_vector_color]], anchor=\"tip\",\n",
        "        name=f'Max Sim. to Pa: {vector_label_with_max_similarity} (head)',\n",
        "        hoverinfo='name'\n",
        "    ))\n",
        "else:\n",
        "    print(\"\\nCould not determine the vector with maximal cosine similarity (check inputs), so not plotting it.\")\n",
        "# --- End of Cosine Similarity Calculation & Plotting Max Sim Vector ---\n",
        "\n",
        "# --- Create and Show Plot ---\n",
        "layout = go.Layout(\n",
        "    title=\"Scatterplot, P-Tet, and Max-Cos-Vec-PCA: 5V_ini\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\", yaxis_title=\"PC 2\", zaxis_title=\"PC 3\",\n",
        "        aspectmode='data',\n",
        "        camera_eye=dict(x=1.8, y=1.8, z=0.5)\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50), # Adjusted top margin for longer title\n",
        "    showlegend=True\n",
        ")\n",
        "fig = go.Figure(data=data_plot, layout=layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kpJO5PnO_c_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">8.1.4 5V_ini: Scatterplot, P-Tet, and Max_Cos_Vec_5Vs</font>"
      ],
      "metadata": {
        "id": "X1RrtewfUrQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot, P-Tet, and Max_Cos_Vec_5V: 5V-ini\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,0\n",
        "P1,0,0,0,1,1\n",
        "P2,0,0.5,0.5,0,0\n",
        "P3,0,1,1,0,0.5\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0.5,1,1,0\n",
        "Cx,0,0,1,0.5,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA AND for 5D similarity (exclude the 'datapoint' column)\n",
        "# X_5d is the original 5-dimensional data\n",
        "X_5d = df.iloc[:, 1:].values\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# --- 1. Cosine Similarity Calculation using Original 5D Data ---\n",
        "print(\"\\n--- 5D Cosine Similarity Calculation (Finding MAXIMAL Similarity) ---\")\n",
        "\n",
        "# Get indices for the relevant data points\n",
        "pa_5d_index = labels.index('Pa')\n",
        "ax_5d_index = labels.index('Ax')\n",
        "bv_5d_index = labels.index('Bv')\n",
        "cx_5d_index = labels.index('Cx')\n",
        "dx_5d_index = labels.index('Dx')\n",
        "\n",
        "# Extract 5D vectors\n",
        "pa_vec_5d = X_5d[pa_5d_index].reshape(1, -1)\n",
        "ax_vec_5d = X_5d[ax_5d_index].reshape(1, -1)\n",
        "bv_vec_5d = X_5d[bv_5d_index].reshape(1, -1)\n",
        "cx_vec_5d = X_5d[cx_5d_index].reshape(1, -1)\n",
        "dx_vec_5d = X_5d[dx_5d_index].reshape(1, -1)\n",
        "\n",
        "vectors_to_check_5d = {\n",
        "    \"Ax\": ax_vec_5d,\n",
        "    \"Bv\": bv_vec_5d,\n",
        "    \"Cx\": cx_vec_5d,\n",
        "    \"Dx\": dx_vec_5d\n",
        "}\n",
        "\n",
        "target_vector_label_5d = \"Pa\"\n",
        "print(f\"Calculating 5D cosine similarity with respect to vector: {target_vector_label_5d} (5D coords: {pa_vec_5d.flatten()})\")\n",
        "\n",
        "# Initialize for finding MAXIMAL similarity\n",
        "max_similarity_5d = -2.0 # Cosine similarity ranges from -1 to 1, so -2 is a safe lower bound for max\n",
        "vector_label_with_max_similarity_5d = None\n",
        "\n",
        "for label_5d, vec_5d in vectors_to_check_5d.items():\n",
        "    similarity_5d = cosine_similarity(vec_5d, pa_vec_5d)[0][0]\n",
        "    print(f\"5D Cosine similarity between {label_5d} and {target_vector_label_5d}: {similarity_5d:.4f}\")\n",
        "    # Check for MAXIMAL similarity\n",
        "    if similarity_5d > max_similarity_5d:\n",
        "        max_similarity_5d = similarity_5d\n",
        "        vector_label_with_max_similarity_5d = label_5d\n",
        "\n",
        "if vector_label_with_max_similarity_5d:\n",
        "    print(f\"\\nThe vector (from 5D data) with the MAXIMAL cosine similarity to {target_vector_label_5d} is: {vector_label_with_max_similarity_5d}\")\n",
        "    print(f\"Maximal 5D cosine similarity: {max_similarity_5d:.4f}\")\n",
        "else:\n",
        "    print(\"\\nCould not determine the vector with maximal 5D cosine similarity.\")\n",
        "\n",
        "\n",
        "# --- 2. Perform PCA for 3D Plotting ---\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data_3d = pca.fit_transform(X_5d) # PCA on the same 5D data\n",
        "\n",
        "# --- 3. Coordinate Extraction for 3D Plotting ---\n",
        "pa_coords_3d = reduced_data_3d[pa_5d_index] # Use original indices\n",
        "p1_coords_3d = reduced_data_3d[labels.index('P1')]\n",
        "p2_coords_3d = reduced_data_3d[labels.index('P2')]\n",
        "p3_coords_3d = reduced_data_3d[labels.index('P3')]\n",
        "q_coords_3d = reduced_data_3d[labels.index('Q?')] # Assuming 'Q?' is the label for Q\n",
        "ax_coords_3d = reduced_data_3d[ax_5d_index] # Use original indices\n",
        "bv_coords_3d = reduced_data_3d[bv_5d_index]\n",
        "cx_coords_3d = reduced_data_3d[cx_5d_index]\n",
        "dx_coords_3d = reduced_data_3d[dx_5d_index]\n",
        "\n",
        "# Store 3D coordinates for easy lookup for plotting the special vector\n",
        "coords_map_3d = {\n",
        "    \"Pa\": pa_coords_3d, \"P1\": p1_coords_3d, \"P2\": p2_coords_3d, \"P3\": p3_coords_3d, \"Q?\": q_coords_3d,\n",
        "    \"Ax\": ax_coords_3d, \"Bv\": bv_coords_3d, \"Cx\": cx_coords_3d, \"Dx\": dx_coords_3d\n",
        "}\n",
        "\n",
        "# --- 4. Plotting Code Setup ---\n",
        "data_plot = []\n",
        "origin_plot = np.array([0, 0, 0])\n",
        "vector_line_width_plot = 4\n",
        "cone_sizeref_plot = 0.2\n",
        "\n",
        "# Scatter plot for all points (using 3D PCA data)\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data_3d[:, 0], y=reduced_data_3d[:, 1], z=reduced_data_3d[:, 2],\n",
        "    mode='markers+text', text=labels, textposition=\"middle right\",\n",
        "    marker=dict(size=4, color='blue'), name='Data Points'\n",
        ")\n",
        "data_plot.append(scatter)\n",
        "\n",
        "# Tetrahedron edges (using 3D PCA data for P1, P2, P3, Q)\n",
        "# Corrected label for Q in initial_vector_points_3d if it's 'Q?'\n",
        "tetrahedron_vertices_3d = np.array([p1_coords_3d, p2_coords_3d, p3_coords_3d, q_coords_3d])\n",
        "tetrahedron_edges = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
        "tetra_lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices_3d[start_index]\n",
        "    end_point = tetrahedron_vertices_3d[end_index]\n",
        "    tetra_lines.append(go.Scatter3d(\n",
        "        x=[start_point[0], end_point[0]], y=[start_point[1], end_point[1]], z=[start_point[2], end_point[2]],\n",
        "        mode='lines', line=dict(color='red', width=3), name=f'Tetra Edge {i+1}', hoverinfo='none'\n",
        "    ))\n",
        "data_plot.extend(tetra_lines)\n",
        "\n",
        "# Initial five vectors (Pa, P1, P2, P3, Q) using their 3D PCA coordinates\n",
        "# Ensuring 'Q?' label is used if that's how Q is defined in initial_vector_points_3d key\n",
        "initial_vector_points_3d = {'Pa': pa_coords_3d, 'P1': p1_coords_3d, 'P2': p2_coords_3d, 'P3': p3_coords_3d, 'Q?': q_coords_3d}\n",
        "initial_vector_color = 'green'\n",
        "\n",
        "for label, p_coords_3d_arr in initial_vector_points_3d.items():\n",
        "    # Shaft\n",
        "    data_plot.append(go.Scatter3d(\n",
        "        x=[origin_plot[0], p_coords_3d_arr[0]], y=[origin_plot[1], p_coords_3d_arr[1]], z=[origin_plot[2], p_coords_3d_arr[2]],\n",
        "        mode='lines', line=dict(color=initial_vector_color, width=vector_line_width_plot),\n",
        "        name=f'Vector to {label} (3D PCA shaft)', hoverinfo='name'\n",
        "    ))\n",
        "    # Head\n",
        "    u_plot, v_plot, w_plot = p_coords_3d_arr - origin_plot\n",
        "    data_plot.append(go.Cone(\n",
        "        x=[p_coords_3d_arr[0]], y=[p_coords_3d_arr[1]], z=[p_coords_3d_arr[2]],\n",
        "        u=[u_plot], v=[v_plot], w=[w_plot],\n",
        "        sizemode=\"absolute\", sizeref=cone_sizeref_plot, showscale=False,\n",
        "        colorscale=[[0, initial_vector_color], [1, initial_vector_color]], anchor=\"tip\",\n",
        "        name=f'Vector to {label} (3D PCA head)', hoverinfo='name'\n",
        "    ))\n",
        "\n",
        "# Plot the vector identified via 5D MAXIMAL similarity, using its 3D PCA coordinates\n",
        "if vector_label_with_max_similarity_5d:\n",
        "    max_sim_point_label = vector_label_with_max_similarity_5d\n",
        "    max_sim_point_coords_3d = coords_map_3d[max_sim_point_label] # Get its 3D PCA coordinates\n",
        "    max_sim_vector_color = 'orange' # Distinct color for MAXIMAL similarity vector\n",
        "\n",
        "    print(f\"Plotting vector for '{max_sim_point_label}' (max 5D sim to Pa) using its 3D PCA coords, color {max_sim_vector_color}\")\n",
        "\n",
        "    # Shaft for max similarity vector (using its 3D PCA coords)\n",
        "    data_plot.append(go.Scatter3d(\n",
        "        x=[origin_plot[0], max_sim_point_coords_3d[0]],\n",
        "        y=[origin_plot[1], max_sim_point_coords_3d[1]],\n",
        "        z=[origin_plot[2], max_sim_point_coords_3d[2]],\n",
        "        mode='lines',\n",
        "        line=dict(color=max_sim_vector_color, width=vector_line_width_plot + 1), # Slightly thicker\n",
        "        name=f'Max 5D Sim. to Pa: {max_sim_point_label} (3D PCA shaft)',\n",
        "        hoverinfo='name'\n",
        "    ))\n",
        "    # Head for max similarity vector (using its 3D PCA coords)\n",
        "    u_max_sim, v_max_sim, w_max_sim = max_sim_point_coords_3d - origin_plot\n",
        "    data_plot.append(go.Cone(\n",
        "        x=[max_sim_point_coords_3d[0]], y=[max_sim_point_coords_3d[1]], z=[max_sim_point_coords_3d[2]],\n",
        "        u=[u_max_sim], v=[v_max_sim], w=[w_max_sim],\n",
        "        sizemode=\"absolute\", sizeref=cone_sizeref_plot, showscale=False,\n",
        "        colorscale=[[0, max_sim_vector_color], [1, max_sim_vector_color]], anchor=\"tip\",\n",
        "        name=f'Max 5D Sim. to Pa: {max_sim_point_label} (3D PCA head)',\n",
        "        hoverinfo='name'\n",
        "    ))\n",
        "else:\n",
        "    print(\"\\nNo maximal similarity vector identified from 5D data to plot.\")\n",
        "\n",
        "# --- Create and Show Plot ---\n",
        "layout = go.Layout(\n",
        "    title=\"Scatterplot, P-Tet, and Max_Cos_Vec_5V: 5V-ini\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\", yaxis_title=\"PC 2\", zaxis_title=\"PC 3\",\n",
        "        aspectmode='data',\n",
        "        camera_eye=dict(x=1.8, y=1.8, z=0.5)\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True\n",
        ")\n",
        "fig = go.Figure(data=data_plot, layout=layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0UuH1HmKCjF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">8.2 Well-Trained 5Vs: CL 1wk + DP/BT>=1month</font>\n",
        "\n",
        "<font color=\"blue\">Below is a typical preception with 1+month training. Correct answer falls directly into the prompt/question subspace.</font>\n",
        "\n",
        "<font color=\"blue\">5Vs---Pa---P1---P2---P3---Q?-|-Ax---Bv---Cx---Dx---</font>\n",
        "\n",
        "<font color=\"blue\">V1-----0-----1-----0------0-----1---|--0-----1-----0-----1---</font>\n",
        "\n",
        "<font color=\"blue\">V2-----1-----0-----1-----1------0---|--0-----0-----0-----0---</font>\n",
        "\n",
        "<font color=\"blue\">V3-----1-----0-----1-----1-----1----|--1-----1-----1-----1---</font>\n",
        "\n",
        "<font color=\"blue\">V4-----0-----1-----0-----0-----1----|--0-----1-----1-----1---</font>\n",
        "\n",
        "<font color=\"blue\">V5-----0-----1-----0-----1-----0----|--0------0-----0-----1---</font>"
      ],
      "metadata": {
        "id": "GiE1M_gtFaj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">8.2.1 5V_fin: Scatterplo and P-Tet</font>"
      ],
      "metadata": {
        "id": "mA75w7aVU6jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaterplot and P-Tet: 5V-fin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,1\n",
        "P1,1,0,0,1,1\n",
        "P2,0,1,1,0,0\n",
        "P3,0,1,1,0,1\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0,1,1,0\n",
        "Cx,0,0,1,1,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "from io import StringIO\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# Get the indices of P1, P2, P3, and Q?\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the 3D scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0],\n",
        "    y=reduced_data[:, 1],\n",
        "    z=reduced_data[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4),\n",
        "    name='Data Points'  # Add a name for the scatter plot\n",
        ")\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}' # Add a name for each line.\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Combine scatter and lines\n",
        "data = [scatter] + lines\n",
        "\n",
        "# Set the title and axis labels\n",
        "layout = go.Layout(\n",
        "    title=\"Scaterplot and P-Tet: 5V-fin\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True #show legend\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "# Show the plot (this will display an interactive plot in Colab)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ampbw-BTFY7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">8.2.2 5V_fin: Scatterplo, P-Tet, and Sphere</font>"
      ],
      "metadata": {
        "id": "r9OQYTBHVFzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot, P-Tet, and P-sphere: 5V-fin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,1\n",
        "P1,1,0,0,1,1\n",
        "P2,0,1,1,0,0\n",
        "P3,0,1,1,0,1\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0,1,1,0\n",
        "Cx,0,0,1,1,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# Get the indices of P1, P2, P3, and Q?\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron (also the points defining the sphere)\n",
        "sphere_defining_points = [p1_coords, p2_coords, p3_coords, q_coords]\n",
        "tetrahedron_vertices = np.array(sphere_defining_points)\n",
        "\n",
        "# --- Functions to calculate sphere center and radius ---\n",
        "def get_sphere_coeffs(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the coefficients A, B, C, D of the sphere equation:\n",
        "    x^2 + y^2 + z^2 + Ax + By + Cz + D = 0\n",
        "    passing through four points p1, p2, p3, p4.\n",
        "    \"\"\"\n",
        "    points = np.array([p1, p2, p3, p4])\n",
        "    # Form matrix M for the system M * [A, B, C, D]' = -[x^2+y^2+z^2]'\n",
        "    M = np.ones((4, 4))\n",
        "    M[:, 0] = points[:, 0]  # x coordinates\n",
        "    M[:, 1] = points[:, 1]  # y coordinates\n",
        "    M[:, 2] = points[:, 2]  # z coordinates\n",
        "\n",
        "    # Right hand side vector\n",
        "    rhs = -(points[:, 0]**2 + points[:, 1]**2 + points[:, 2]**2)\n",
        "\n",
        "    try:\n",
        "        # Solve for A, B, C, D\n",
        "        coeffs = np.linalg.solve(M, rhs)\n",
        "        return coeffs\n",
        "    except np.linalg.LinAlgError:\n",
        "        print(\"Error: The four points might be coplanar. Cannot determine a unique sphere.\")\n",
        "        return None\n",
        "\n",
        "def calculate_sphere_center_radius(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the center and radius of a sphere passing through four points.\n",
        "    p1, p2, p3, p4 are 3D points as NumPy arrays or lists [x, y, z].\n",
        "    Returns (center, radius) or (None, None) if points are coplanar.\n",
        "    \"\"\"\n",
        "    coeffs = get_sphere_coeffs(np.array(p1), np.array(p2), np.array(p3), np.array(p4))\n",
        "\n",
        "    if coeffs is None:\n",
        "        return None, None\n",
        "\n",
        "    A, B, C, D_coeff = coeffs\n",
        "\n",
        "    # Center (xc, yc, zc)\n",
        "    xc = -A / 2\n",
        "    yc = -B / 2\n",
        "    zc = -C / 2\n",
        "    center = np.array([xc, yc, zc])\n",
        "\n",
        "    # Radius R\n",
        "    # R^2 = xc^2 + yc^2 + zc^2 - D_coeff\n",
        "    radius_sq = xc**2 + yc**2 + zc**2 - D_coeff\n",
        "    if radius_sq < 0:\n",
        "        print(\"Error: Calculated radius squared is negative (points might be collinear or an issue with PCA reduction).\")\n",
        "        return None, None\n",
        "    radius = np.sqrt(radius_sq)\n",
        "\n",
        "    return center, radius\n",
        "\n",
        "# --- Function to generate Plotly sphere surface ---\n",
        "def get_plotly_sphere_surface(center, radius, color='rgba(0,180,255,0.3)', resolution=50, name='Circumsphere'):\n",
        "    \"\"\"\n",
        "    Generates Plotly go.Surface data for a sphere.\n",
        "    center: NumPy array or list for the sphere's center [xc, yc, zc].\n",
        "    radius: Radius of the sphere.\n",
        "    color: Color of the sphere as an rgba string (e.g., 'rgba(R,G,B,A)').\n",
        "    resolution: Number of points for theta and phi.\n",
        "    name: Legend name for the sphere.\n",
        "    \"\"\"\n",
        "    theta = np.linspace(0, 2 * np.pi, resolution)\n",
        "    phi = np.linspace(0, np.pi, resolution)\n",
        "    theta, phi = np.meshgrid(theta, phi)\n",
        "\n",
        "    x = center[0] + radius * np.cos(theta) * np.sin(phi)\n",
        "    y = center[1] + radius * np.sin(theta) * np.sin(phi)\n",
        "    z = center[2] + radius * np.cos(phi)\n",
        "\n",
        "    # To achieve a single color with opacity, we set the color directly in the colorscale\n",
        "    # and ensure opacity is handled by the color string itself or the opacity property of go.Surface\n",
        "\n",
        "    # Extract RGB from the rgba string if provided, default to a blue if format is unexpected\n",
        "    try:\n",
        "        rgb_color_part = color.split('(')[1].split(')')[0].split(',')\n",
        "        r, g, b = rgb_color_part[0], rgb_color_part[1], rgb_color_part[2]\n",
        "        plotly_color = f'rgb({r},{g},{b})'\n",
        "        opacity_val = float(rgb_color_part[3]) if len(rgb_color_part) > 3 else 0.3\n",
        "    except:\n",
        "        plotly_color = 'rgb(0,180,255)' # Default blue\n",
        "        opacity_val = 0.3\n",
        "\n",
        "    return go.Surface(\n",
        "        x=x, y=y, z=z,\n",
        "        colorscale=[[0, plotly_color], [1, plotly_color]], # Solid color\n",
        "        showscale=False,\n",
        "        opacity=opacity_val,\n",
        "        name=name,\n",
        "        hoverinfo='skip' # Optional: disable hover for the sphere surface\n",
        "    )\n",
        "\n",
        "# --- Plotting ---\n",
        "\n",
        "# Create the 3D scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0],\n",
        "    y=reduced_data[:, 1],\n",
        "    z=reduced_data[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4, color='black'), # Changed marker color for better visibility against sphere\n",
        "    name='Data Points'\n",
        ")\n",
        "\n",
        "# Define the edges of the tetrahedron\n",
        "tetrahedron_edges_indices = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1\n",
        "    (1, 2), (1, 3),          # Edges from P2\n",
        "    (2, 3)                   # Edge from P3\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_idx, end_idx) in enumerate(tetrahedron_edges_indices):\n",
        "    start_point = tetrahedron_vertices[start_idx]\n",
        "    end_point = tetrahedron_vertices[end_idx]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=3),\n",
        "            name=f'Tetrahedron Edge' # Simplified name\n",
        "        )\n",
        "    )\n",
        "# To avoid multiple \"Tetrahedron Edge\" legends, only the first one will show by default if names are identical.\n",
        "# Or, make them unique if needed, or group them. For simplicity, keep as is or assign name to only one.\n",
        "if lines:\n",
        "    lines[0].showlegend = True # Show legend for the first edge only as representative\n",
        "    for line_trace in lines[1:]:\n",
        "        line_trace.showlegend = False\n",
        "\n",
        "\n",
        "# Calculate sphere center and radius using the PCA-reduced coordinates\n",
        "sphere_center, sphere_radius = calculate_sphere_center_radius(p1_coords, p2_coords, p3_coords, q_coords)\n",
        "\n",
        "# Initialize data list for the figure\n",
        "data_traces = [scatter] + lines\n",
        "\n",
        "# Add sphere to the plot if calculation was successful\n",
        "sphere_trace = None\n",
        "if sphere_center is not None and sphere_radius is not None:\n",
        "    print(f\"Sphere Center (PCA coords): {sphere_center}\")\n",
        "    print(f\"Sphere Radius (PCA coords): {sphere_radius}\")\n",
        "    sphere_trace = get_plotly_sphere_surface(\n",
        "        sphere_center,\n",
        "        sphere_radius,\n",
        "        color='rgba(100, 180, 255, 0.3)', # Light blue, semi-transparent\n",
        "        resolution=40, # Lower resolution for faster rendering, increase for smoother sphere\n",
        "        name='Circumsphere P1-P2-P3-Q?'\n",
        "    )\n",
        "    data_traces.append(sphere_trace)\n",
        "else:\n",
        "    print(\"Could not calculate sphere parameters. Sphere will not be plotted.\")\n",
        "\n",
        "\n",
        "# Set the title and axis labels\n",
        "layout = go.Layout(\n",
        "    title=\"Scatterplot, P-Tet, and P-sphere: 5V-fin\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "        aspectmode='data' # 'data', 'cube', 'auto', 'manual'\n",
        "                         # 'data' ensures that the scaling of axes matches the data range\n",
        "                         # 'cube' makes the plot region a cube\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=data_traces, layout=layout)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QkzzUSD8HgwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">8.2.3 5V_fin: Scatterplot, P-Tet, and Max_Cos_Vec_5Vs</font>"
      ],
      "metadata": {
        "id": "eWg9RQCCVLzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaterplot, P-Tet, and Max_Cos_Vec_5V: 5V-fin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from io import StringIO\n",
        "\n",
        "# Your new data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,1\n",
        "P1,1,0,0,1,1\n",
        "P2,0,1,1,0,0\n",
        "P3,0,1,1,0,1\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0,1,1,0\n",
        "Cx,0,0,1,1,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA AND for 5D similarity (exclude the 'datapoint' column)\n",
        "# X_5d is the original 5-dimensional data\n",
        "X_5d = df.iloc[:, 1:].values\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# --- 1. Cosine Similarity Calculation using Original 5D Data ---\n",
        "print(\"\\n--- 5D Cosine Similarity Calculation ---\")\n",
        "\n",
        "# Get indices for the relevant data points\n",
        "pa_5d_index = labels.index('Pa')\n",
        "ax_5d_index = labels.index('Ax')\n",
        "bv_5d_index = labels.index('Bv')\n",
        "cx_5d_index = labels.index('Cx')\n",
        "dx_5d_index = labels.index('Dx')\n",
        "\n",
        "# Extract 5D vectors\n",
        "pa_vec_5d = X_5d[pa_5d_index].reshape(1, -1)\n",
        "ax_vec_5d = X_5d[ax_5d_index].reshape(1, -1)\n",
        "bv_vec_5d = X_5d[bv_5d_index].reshape(1, -1)\n",
        "cx_vec_5d = X_5d[cx_5d_index].reshape(1, -1)\n",
        "dx_vec_5d = X_5d[dx_5d_index].reshape(1, -1)\n",
        "\n",
        "vectors_to_check_5d = {\n",
        "    \"Ax\": ax_vec_5d,\n",
        "    \"Bv\": bv_vec_5d,\n",
        "    \"Cx\": cx_vec_5d,\n",
        "    \"Dx\": dx_vec_5d\n",
        "}\n",
        "\n",
        "target_vector_label_5d = \"Pa\"\n",
        "print(f\"Calculating 5D cosine similarity with respect to vector: {target_vector_label_5d} (5D coords: {pa_vec_5d.flatten()})\")\n",
        "\n",
        "max_similarity_5d = -2.0 # Cosine similarity ranges from -1 to 1, so 2 is a safe upper bound for max\n",
        "vector_label_with_max_similarity_5d = None\n",
        "\n",
        "for label_5d, vec_5d in vectors_to_check_5d.items():\n",
        "    similarity_5d = cosine_similarity(vec_5d, pa_vec_5d)[0][0]\n",
        "    print(f\"5D Cosine similarity between {label_5d} and {target_vector_label_5d}: {similarity_5d:.4f}\")\n",
        "    if similarity_5d > max_similarity_5d:\n",
        "        max_similarity_5d = similarity_5d\n",
        "        vector_label_with_max_similarity_5d = label_5d\n",
        "\n",
        "if vector_label_with_max_similarity_5d:\n",
        "    print(f\"\\nThe vector (from 5D data) with the MAXIMAL cosine similarity to {target_vector_label_5d} is: {vector_label_with_max_similarity_5d}\")\n",
        "    print(f\"Maximal 5D cosine similarity: {max_similarity_5d:.4f}\")\n",
        "else:\n",
        "    print(\"\\nCould not determine the vector with maximal 5D cosine similarity.\")\n",
        "\n",
        "\n",
        "# --- 2. Perform PCA for 3D Plotting ---\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data_3d = pca.fit_transform(X_5d) # PCA on the same 5D data\n",
        "\n",
        "# --- 3. Coordinate Extraction for 3D Plotting ---\n",
        "pa_coords_3d = reduced_data_3d[pa_5d_index] # Use original indices\n",
        "p1_coords_3d = reduced_data_3d[labels.index('P1')]\n",
        "p2_coords_3d = reduced_data_3d[labels.index('P2')]\n",
        "p3_coords_3d = reduced_data_3d[labels.index('P3')]\n",
        "q_coords_3d = reduced_data_3d[labels.index('Q?')]\n",
        "ax_coords_3d = reduced_data_3d[ax_5d_index] # Use original indices\n",
        "bv_coords_3d = reduced_data_3d[bv_5d_index]\n",
        "cx_coords_3d = reduced_data_3d[cx_5d_index]\n",
        "dx_coords_3d = reduced_data_3d[dx_5d_index]\n",
        "\n",
        "# Store 3D coordinates for easy lookup for plotting the special vector\n",
        "coords_map_3d = {\n",
        "    \"Pa\": pa_coords_3d, \"P1\": p1_coords_3d, \"P2\": p2_coords_3d, \"P3\": p3_coords_3d, \"Q?\": q_coords_3d,\n",
        "    \"Ax\": ax_coords_3d, \"Bv\": bv_coords_3d, \"Cx\": cx_coords_3d, \"Dx\": dx_coords_3d\n",
        "}\n",
        "\n",
        "# --- 4. Plotting Code Setup ---\n",
        "data_plot = []\n",
        "origin_plot = np.array([0, 0, 0])\n",
        "vector_line_width_plot = 4\n",
        "cone_sizeref_plot = 0.2\n",
        "\n",
        "# Scatter plot for all points (using 3D PCA data)\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data_3d[:, 0], y=reduced_data_3d[:, 1], z=reduced_data_3d[:, 2],\n",
        "    mode='markers+text', text=labels, textposition=\"middle right\",\n",
        "    marker=dict(size=4, color='blue'), name='Data Points'\n",
        ")\n",
        "data_plot.append(scatter)\n",
        "\n",
        "# Tetrahedron edges (using 3D PCA data for P1, P2, P3, Q)\n",
        "tetrahedron_vertices_3d = np.array([p1_coords_3d, p2_coords_3d, p3_coords_3d, q_coords_3d])\n",
        "tetrahedron_edges = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
        "tetra_lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices_3d[start_index]\n",
        "    end_point = tetrahedron_vertices_3d[end_index]\n",
        "    tetra_lines.append(go.Scatter3d(\n",
        "        x=[start_point[0], end_point[0]], y=[start_point[1], end_point[1]], z=[start_point[2], end_point[2]],\n",
        "        mode='lines', line=dict(color='red', width=3), name=f'Tetra Edge {i+1}', hoverinfo='none'\n",
        "    ))\n",
        "data_plot.extend(tetra_lines)\n",
        "\n",
        "# Initial five vectors (Pa, P1, P2, P3, Q) using their 3D PCA coordinates\n",
        "initial_vector_points_3d = {'Pa': pa_coords_3d, 'P1': p1_coords_3d, 'P2': p2_coords_3d, 'P3': p3_coords_3d, 'Q': q_coords_3d}\n",
        "initial_vector_color = 'green'\n",
        "\n",
        "for label, p_coords_3d_arr in initial_vector_points_3d.items():\n",
        "    # Shaft\n",
        "    data_plot.append(go.Scatter3d(\n",
        "        x=[origin_plot[0], p_coords_3d_arr[0]], y=[origin_plot[1], p_coords_3d_arr[1]], z=[origin_plot[2], p_coords_3d_arr[2]],\n",
        "        mode='lines', line=dict(color=initial_vector_color, width=vector_line_width_plot),\n",
        "        name=f'Vector to {label} (3D PCA shaft)', hoverinfo='name'\n",
        "    ))\n",
        "    # Head\n",
        "    u_plot, v_plot, w_plot = p_coords_3d_arr - origin_plot\n",
        "    data_plot.append(go.Cone(\n",
        "        x=[p_coords_3d_arr[0]], y=[p_coords_3d_arr[1]], z=[p_coords_3d_arr[2]],\n",
        "        u=[u_plot], v=[v_plot], w=[w_plot],\n",
        "        sizemode=\"absolute\", sizeref=cone_sizeref_plot, showscale=False,\n",
        "        colorscale=[[0, initial_vector_color], [1, initial_vector_color]], anchor=\"tip\",\n",
        "        name=f'Vector to {label} (3D PCA head)', hoverinfo='name'\n",
        "    ))\n",
        "\n",
        "# Plot the vector identified via 5D maximal similarity, using its 3D PCA coordinates\n",
        "if vector_label_with_max_similarity_5d:\n",
        "    max_sim_point_label = vector_label_with_max_similarity_5d\n",
        "    max_sim_point_coords_3d = coords_map_3d[max_sim_point_label] # Get its 3D PCA coordinates\n",
        "    max_sim_vector_color = 'magenta' # Distinct color\n",
        "\n",
        "    print(f\"Plotting vector for '{max_sim_point_label}' (max 5D sim to Pa) using its 3D PCA coords, color {max_sim_vector_color}\")\n",
        "\n",
        "    # Shaft for max similarity vector (using its 3D PCA coords)\n",
        "    data_plot.append(go.Scatter3d(\n",
        "        x=[origin_plot[0], max_sim_point_coords_3d[0]],\n",
        "        y=[origin_plot[1], max_sim_point_coords_3d[1]],\n",
        "        z=[origin_plot[2], max_sim_point_coords_3d[2]],\n",
        "        mode='lines',\n",
        "        line=dict(color=max_sim_vector_color, width=vector_line_width_plot + 1), # Slightly thicker\n",
        "        name=f'Max 5D Sim. to Pa: {max_sim_point_label} (3D PCA shaft)',\n",
        "        hoverinfo='name'\n",
        "    ))\n",
        "    # Head for max similarity vector (using its 3D PCA coords)\n",
        "    u_max_sim, v_max_sim, w_max_sim = max_sim_point_coords_3d - origin_plot\n",
        "    data_plot.append(go.Cone(\n",
        "        x=[max_sim_point_coords_3d[0]], y=[max_sim_point_coords_3d[1]], z=[max_sim_point_coords_3d[2]],\n",
        "        u=[u_max_sim], v=[v_max_sim], w=[w_max_sim],\n",
        "        sizemode=\"absolute\", sizeref=cone_sizeref_plot, showscale=False,\n",
        "        colorscale=[[0, max_sim_vector_color], [1, max_sim_vector_color]], anchor=\"tip\",\n",
        "        name=f'Max 5D Sim. to Pa: {max_sim_point_label} (3D PCA head)',\n",
        "        hoverinfo='name'\n",
        "    ))\n",
        "else:\n",
        "    print(\"\\nNo maximal similarity vector identified from 5D data to plot.\")\n",
        "\n",
        "# --- Create and Show Plot ---\n",
        "layout = go.Layout(\n",
        "    title=\"Scaterplot, P-Tet, and Max_Cos_Vec_5V: 5V-fin\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\", yaxis_title=\"PC 2\", zaxis_title=\"PC 3\",\n",
        "        aspectmode='data',\n",
        "        camera_eye=dict(x=1.8, y=1.8, z=0.5)\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50),\n",
        "    showlegend=True\n",
        ")\n",
        "fig = go.Figure(data=data_plot, layout=layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0lTtD-1vV4Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">8.2.4 5V_fin: Scatterplo, P-Tet, and Max_Cos_Vec_PCA</font>"
      ],
      "metadata": {
        "id": "qvz5eNM6VtkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot, P-Tet, and Max_Cos_Vec_PCA: 5V-fin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,1\n",
        "P1,1,0,0,1,1\n",
        "P2,0,1,1,0,0\n",
        "P3,0,1,1,0,1\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0,1,1,0\n",
        "Cx,0,0,1,1,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# --- Coordinate Extraction ---\n",
        "pa_index = labels.index('Pa')\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "ax_index = labels.index('Ax')\n",
        "bv_index = labels.index('Bv')\n",
        "cx_index = labels.index('Cx')\n",
        "dx_index = labels.index('Dx')\n",
        "\n",
        "pa_coords = reduced_data[pa_index]\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "ax_coords = reduced_data[ax_index]\n",
        "bv_coords = reduced_data[bv_index]\n",
        "cx_coords = reduced_data[cx_index]\n",
        "dx_coords = reduced_data[dx_index]\n",
        "\n",
        "# --- Plotting Code Setup ---\n",
        "data_plot = [] # Initialize list for all plot traces\n",
        "\n",
        "# Scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0], y=reduced_data[:, 1], z=reduced_data[:, 2],\n",
        "    mode='markers+text', text=labels, textposition=\"middle right\",\n",
        "    marker=dict(size=4, color='blue'), name='Data Points'\n",
        ")\n",
        "data_plot.append(scatter)\n",
        "\n",
        "# Tetrahedron edges\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "tetrahedron_edges = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
        "tetra_lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    tetra_lines.append(go.Scatter3d(\n",
        "        x=[start_point[0], end_point[0]], y=[start_point[1], end_point[1]], z=[start_point[2], end_point[2]],\n",
        "        mode='lines', line=dict(color='red', width=3), name=f'Tetra Edge {i+1}', hoverinfo='none'\n",
        "    ))\n",
        "data_plot.extend(tetra_lines)\n",
        "\n",
        "# Initial five vectors (Pa, P1, P2, P3, Q)\n",
        "vector_points_coords_for_plot = {'Pa': pa_coords, 'P1': p1_coords, 'P2': p2_coords, 'P3': p3_coords, 'Q': q_coords}\n",
        "origin_plot = np.array([0, 0, 0])\n",
        "initial_vector_color = 'green'\n",
        "vector_line_width_plot = 4\n",
        "cone_sizeref_plot = 0.2  # Adjust as needed\n",
        "\n",
        "for label, p_coords_arr in vector_points_coords_for_plot.items():\n",
        "    p_coords_plot = np.array(p_coords_arr)\n",
        "    # Shaft\n",
        "    data_plot.append(go.Scatter3d(\n",
        "        x=[origin_plot[0], p_coords_plot[0]], y=[origin_plot[1], p_coords_plot[1]], z=[origin_plot[2], p_coords_plot[2]],\n",
        "        mode='lines', line=dict(color=initial_vector_color, width=vector_line_width_plot),\n",
        "        name=f'Vector to {label} (shaft)', hoverinfo='name'\n",
        "    ))\n",
        "    # Head\n",
        "    u_plot, v_plot, w_plot = p_coords_plot - origin_plot\n",
        "    data_plot.append(go.Cone(\n",
        "        x=[p_coords_plot[0]], y=[p_coords_plot[1]], z=[p_coords_plot[2]],\n",
        "        u=[u_plot], v=[v_plot], w=[w_plot],\n",
        "        sizemode=\"absolute\", sizeref=cone_sizeref_plot, showscale=False,\n",
        "        colorscale=[[0, initial_vector_color], [1, initial_vector_color]], anchor=\"tip\",\n",
        "        name=f'Vector to {label} (head)', hoverinfo='name'\n",
        "    ))\n",
        "\n",
        "# --- Cosine Similarity Calculation ---\n",
        "print(\"\\n--- Cosine Similarity Calculation ---\")\n",
        "vec_ax = ax_coords.reshape(1, -1)\n",
        "vec_bv = bv_coords.reshape(1, -1)\n",
        "vec_cx = cx_coords.reshape(1, -1)\n",
        "vec_dx = dx_coords.reshape(1, -1)\n",
        "\n",
        "# Store coordinates for Ax, Bv, Cx, Dx for easy lookup later\n",
        "coords_map = {\n",
        "    \"Ax\": ax_coords,\n",
        "    \"Bv\": bv_coords,\n",
        "    \"Cx\": cx_coords,\n",
        "    \"Dx\": dx_coords\n",
        "}\n",
        "vectors_to_check_for_sim = {\n",
        "    \"Ax\": vec_ax,\n",
        "    \"Bv\": vec_bv,\n",
        "    \"Cx\": vec_cx,\n",
        "    \"Dx\": vec_dx\n",
        "}\n",
        "\n",
        "target_vector_label = \"Pa\"\n",
        "target_vector_coords_for_sim = pa_coords.reshape(1, -1)\n",
        "\n",
        "print(f\"Calculating cosine similarity with respect to vector: {target_vector_label} (coordinates: {target_vector_coords_for_sim.flatten()})\")\n",
        "\n",
        "max_similarity = -2\n",
        "vector_label_with_max_similarity = None\n",
        "\n",
        "for label, vec in vectors_to_check_for_sim.items():\n",
        "    similarity = cosine_similarity(vec, target_vector_coords_for_sim)[0][0]\n",
        "    print(f\"Cosine similarity between {label} and {target_vector_label}: {similarity:.4f}\")\n",
        "    if similarity > max_similarity:\n",
        "        max_similarity = similarity\n",
        "        vector_label_with_max_similarity = label\n",
        "\n",
        "if vector_label_with_max_similarity:\n",
        "    print(f\"\\nThe vector from (Ax, Bv, Cx, Dx) with the maximal cosine similarity to {target_vector_label} is: {vector_label_with_max_similarity}\")\n",
        "    print(f\"Maximal cosine similarity: {max_similarity:.4f}\")\n",
        "\n",
        "    # --- Plot the vector with maximal cosine similarity ---\n",
        "    max_sim_coords = coords_map[vector_label_with_max_similarity]\n",
        "    max_sim_vector_color = 'magenta' # Distinct color for this vector\n",
        "\n",
        "    print(f\"Plotting maximal similarity vector: {vector_label_with_max_similarity} with color {max_sim_vector_color}\")\n",
        "\n",
        "    # Shaft for max similarity vector\n",
        "    data_plot.append(go.Scatter3d(\n",
        "        x=[origin_plot[0], max_sim_coords[0]],\n",
        "        y=[origin_plot[1], max_sim_coords[1]],\n",
        "        z=[origin_plot[2], max_sim_coords[2]],\n",
        "        mode='lines',\n",
        "        line=dict(color=max_sim_vector_color, width=vector_line_width_plot + 1), # Slightly thicker\n",
        "        name=f'Max Sim. to Pa: {vector_label_with_max_similarity} (shaft)',\n",
        "        hoverinfo='name'\n",
        "    ))\n",
        "    # Head for max similarity vector\n",
        "    u_max_sim, v_max_sim, w_max_sim = max_sim_coords - origin_plot\n",
        "    data_plot.append(go.Cone(\n",
        "        x=[max_sim_coords[0]], y=[max_sim_coords[1]], z=[max_sim_coords[2]],\n",
        "        u=[u_max_sim], v=[v_max_sim], w=[w_max_sim],\n",
        "        sizemode=\"absolute\", sizeref=cone_sizeref_plot, showscale=False,\n",
        "        colorscale=[[0, max_sim_vector_color], [1, max_sim_vector_color]], anchor=\"tip\",\n",
        "        name=f'Max Sim. to Pa: {vector_label_with_max_similarity} (head)',\n",
        "        hoverinfo='name'\n",
        "    ))\n",
        "else:\n",
        "    print(\"\\nCould not determine the vector with maximal cosine similarity (check inputs), so not plotting it.\")\n",
        "# --- End of Cosine Similarity Calculation & Plotting Max Sim Vector ---\n",
        "\n",
        "# --- Create and Show Plot ---\n",
        "layout = go.Layout(\n",
        "    title=\"Scatterplot, P-Tet, and Max_Cos_Vec_PCA: 5V-fin\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\", yaxis_title=\"PC 2\", zaxis_title=\"PC 3\",\n",
        "        aspectmode='data',\n",
        "        camera_eye=dict(x=1.8, y=1.8, z=0.5)\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=50), # Adjusted top margin for longer title\n",
        "    showlegend=True\n",
        ")\n",
        "fig = go.Figure(data=data_plot, layout=layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0a7cNLJK_3ee"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "LIT in Notebooks",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}