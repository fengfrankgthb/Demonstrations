{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fengfrankgthb/Demonstrations/blob/main/LIT_CeO2_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2c6PyqQaNiA"
      },
      "source": [
        "# SAT & GRE tested w/ all-mpnet-base-v2 Model 2025.05.18\n",
        "\n",
        "In this project, an well-trained **transformer model** is used to represent an knowledgable super-student in front of SAT and GRE reading quexstions. **Biases** and **confusions** are disclosed, likely attributable to **over-fitting** to the training of the model. Over-fitting is the ML terminology for **test cramming**, a phenominon when fitting specificities of **training dataset** caused the model to not being able to fit to specificities of **testing dataset**.\n",
        "\n",
        "## 1. Install Necessary Libraries\n",
        "* **sentence-transformers** This is the text embedding library\n",
        "* **scikit-learn** This is the machine learning library\n",
        "* **matplotlib** This is the mat-lab style plotting library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers scikit-learn matplotlib"
      ],
      "metadata": {
        "id": "158mSJto-mYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. import necessary modules from the libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "jd2Yqvua_ktp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mpl_toolkits.mplot3d**: matplot 3D plotting lib\n",
        "\n",
        "**numpy**: Numerical Python, the fundamental python lib\n",
        "\n",
        "**Axes3D** 3D plotting class\n",
        "\n",
        "**PCA** Principal Components Analysis for linear dimension reduction.\n",
        "\n",
        "**TSNE** t-SNE non-linear dimension reduction to creat more scattered effect"
      ],
      "metadata": {
        "id": "XPORFA-auaqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "# set matplot to inline (static) mode, or notebook for interactive mode)\n",
        "# even though default is inline mode, be explicit to avoid any confusion\n",
        "# the interactive notebook mode is often unstable at colab environment\n",
        "# alternative is set as 'inline' and use 'plotly'.\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "d8ArIZFq_uxp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Imput text data\n",
        "\n",
        "Choose one subsection below:\n",
        "\n",
        "### 3.1 Example 1: CeO2-NPs, All-Mighty vs 5Vs (Section 8)\n",
        "\n",
        "Used **118HHH Q1 on CeO2-NPs** for illustration, breaking into 9 components:\n",
        "\n",
        "* Pa = All Sentences combined in Passage\n",
        "* P1 = 1st sentence in Passage\n",
        "* P2 = 2nd sentence in Passage\n",
        "* P3 = 3rd sentence in Passage\n",
        "* Q? = the Question sentence\n",
        "* Ax = wrong choice A\n",
        "* Bv = correct answer B\n",
        "* Cx = wrong choice C\n",
        "* Dx = wrong choice D"
      ],
      "metadata": {
        "id": "6NTtMzbl_5f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: CeO2 Question\n",
        "# This is a *** question used to directly compare with 5Vs in section 8.1 and 8.2\n",
        "sentences = [\"Pa: Some fuel additives contain cerium oxide nanoparticles (CeO2-NPs), which can leach into waterways and soils via waste water. In a 2015 study Mael Garaud and colleagues found that CeO2-NPs can accumulate in the bodies of zebra mussels (Dreissena polymorpha). While bioaccumulation of manufactured nanoparticles may be inherently worrisome, it has been hypothesized that CeO2-NPs bioaccumulation in invertebrate like D. polymorpha could serve a valuable proxy role, observing the need for manufacturers to conduct costly and intrusive sampling of vertebrate species--such as rainbow trout (Oncorhynchus mykiss), commonly used in regulatory compliance testing--for manipulative bioaccumulation, as environmental protection laws currently require.\",\n",
        "    \"P1: Some fuel additives contain cerium oxide nanoparticles (CeO2-NPs), which can leach into waterways and soils via waste water.\",\n",
        "    \"P2: In a 2015 study Mael Garaud and colleagues found that CeO2-NPs can accumulate in the bodies of zebra mussels (Dreissena polymorpha).\",\n",
        "    \"P3: While bioaccumulation of manufactured nanoparticles may be inherently worrisome, it has been hypothesized that CeO2-NPs bioaccumulation in invertebrate like D. polymorpha could serve a valuable proxy role, observing the need for manufacturers to conduct costly and intrusive sampling of vertebrate species--such as rainbow trout (Oncorhynchus mykiss), commonly used in regulatory compliance testing--for manipulative bioaccumulation, as environmental protection laws currently require.\",\n",
        "    \"Q?: Which finding, if true, would most directly weaken the hypothesis presented in the text?\",\n",
        "    \"Ax) When D. polymorpha and O. mykiss are exposed to similar levels of CeO2-NPs, concentrations of CeO2-NPs in animals of both species show little variation from individual to individual.\",\n",
        "    \"Bv) The rate of CeO2-NPs uptake in D. polymorpha differs from the rate of CeO2-NPs uptake in O. mykiss in a way that is not yet well understood by researchers.\",\n",
        "    \"Cx) D.polymorpha has been shown to accumulate several other types of manufactured nanoparticles in addition to CeO2-NPs, whereas O. mykiss has been shown to accumulate only CeO2-NPs.\",\n",
        "    \"Dx) Compared with O. mykiss, D.polymorpha can accumulate detectable CeO2-NPs concentrations with significantly fewer negative effects.\"\n",
        "]"
      ],
      "metadata": {
        "id": "xFnLqNPQ_-54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Examples 2-3-4-5 & Alternatives:\n",
        "\n",
        "* **Queen Egypt**: demonstrates **omis bias**.\n",
        "* **LNH Study**: demonstrates **perif bias**, **long-comp confusion**, **num-cont confusion** (V3), and **mul-rel confusion**(V4). In the end, **num-cont confusion** (V3), and **mul-rel confusion**(V4) of 5Vs are combined to demonstrate the clear solution effect.\n",
        "* **Cotton Mather**; GRE reading question demonstrates **long-pass confusion** and **spec-confusion** in **graduate level** texts."
      ],
      "metadata": {
        "id": "WbrBMrb36qc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Queen Egypt\n",
        "# This is ** question used to discover omis bias among choices\n",
        "sentences = [\"Pa: Archaeologist Christiana Kohler and her team excavated the Egyptian tomb of Queen Merneith, the wife of a First Dynasty pharaoh. Some scholars claim that she also ruled Egypt on her own and was actually the first female pharaoh. The team found a tablet in Merneith’s tomb with writing suggesting that she was in charge of the country’s treasury and other central offices. Whether Merneith was a pharaoh or not, this discovery supports the idea that Merneith likely _______\",\n",
        "    \"P1: Archaeologist Christiana Kohler and her team excavated the Egyptian tomb of Queen Merneith, the wife of a First Dynasty pharaoh.\",\n",
        "    \"P2: Some scholars claim that she also ruled Egypt on her own and was actually the first female pharaoh.\",\n",
        "    \"P3: The team found a tablet in Merneith’s tomb with writing suggesting that she was in charge of the country’s treasury and other central offices.\",\n",
        "    \"Q?: Whether Merneith was a pharaoh or not, this discovery supports the idea that Merneith likely _______ (choose from Av, Bx, Cx, and Dx). \",\n",
        "    \"Av) had an important role in Egypt’s government.\",\n",
        "    \"Bx) lived after rather than before the First Dynasty of Egypt.\",\n",
        "    \"Cx) traveled beyond Egypt’s borders often.\",\n",
        "    \"Dx) created a new form of writing in Egypt.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "X9rF5XJaasNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 (Alt-1): Queen Egypt amended\n",
        "# This is the revised the example 2 to solve omis bias among choices.\n",
        "sentences = [\"Pa: Archaeologist Christiana Kohler and her team excavated the Egyptian tomb of Queen Merneith, the wife of a First Dynasty pharaoh. Some scholars claim that she also ruled Egypt on her own and was actually the first female pharaoh. The team found a tablet in Merneith’s tomb with writing suggesting that she was in charge of the country’s treasury and other central offices. Whether Merneith was a pharaoh or not, this discovery supports the idea that Merneith likely _______\",\n",
        "    \"P1: Archaeologist Christiana Kohler and her team excavated the Egyptian tomb of Queen Merneith, the wife of a First Dynasty pharaoh.\",\n",
        "    \"P2: Some scholars claim that she also ruled Egypt on her own and was actually the first female pharaoh.\",\n",
        "    \"P3: The team found a tablet in Merneith’s tomb with writing suggesting that she was in charge of the country’s treasury and other central offices.\",\n",
        "    \"Q?: This discovery supports that _______ (choose from Av, Bx, Cx, and Dx)?\",\n",
        "    \"Av) Merneith likely had a role in her government\",\n",
        "    \"Bx) Merneith likely lived after the First Dynasty\",\n",
        "    \"Cx) Merneith likely traveld beyond Egypt borders\",\n",
        "    \"Dx) Merneith likely created an Egptian writing.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "T5uZUh9kj0jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: LNH Study\n",
        "# This is a *** question used to discover perif bias, long-comp confusion, num-cont confusion, and mul-rel confusion in text.\n",
        "sentences = [\"Pa: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules. Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"P1: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules.\",\n",
        "    \"P2: Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"Pa: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules. Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’ claim?\",\n",
        "    \"Ax) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and fusion and between grammatical complexity and informativity\",\n",
        "    \"Bx) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and exotericity.\",\n",
        "    \"Cx) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical complexity and fusion.\",\n",
        "    \"Dv) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between fusion and exotericity and between informativity and exotericity.\"\n",
        "]"
      ],
      "metadata": {
        "id": "Uv8etHnIkoGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-1): LNH Study\n",
        "# This is the revised Example 3 to solve perif bias in text.\n",
        "sentences = [\"Pa: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules. Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"P1: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules.\",\n",
        "    \"P2: Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"Pa: The linguistic niche hypothesis (LNH) posits that the exotericity of languages (how prevalent non-native speakers are) and grammatical complexity are inversely related, which the LNH ascribes to attrition of complex grammatical rules as more non-native speakers adopt the language but fail to acquire those rules. Focusing on two characteristics that are positive indices of grammatical complexity, fusion (when new phonemes arise from the merger of previously distinct ones) and informativity (languages’ capacity for meaningful variation), Olena Shcherbakova and colleagues conducted a quantitative analysis for more than 1,300 languages and claim the outcome is inconsistent with the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’ claim?\",\n",
        "    \"Ax) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and fusion and between grammatical complexity and informativity\",\n",
        "    \"Bx) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and exotericity.\",\n",
        "    \"Cx) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical complexity and fusion.\",\n",
        "    \"Dv) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical fusion and exotericity and between grammatical informativity and exotericity.\"\n",
        "]"
      ],
      "metadata": {
        "id": "vfZEp-mVA-O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-2): LNH Study\n",
        "# This is the revised Example 3 to solve long-comp confusion in text.\n",
        "sentences = [\"Pa: The Linguistic Niche Hypothesis (LNH) suggests that as more non-native speakers adopt a language, its grammatical complexity decreases. This occurs because these speakers often do not fully acquire complex grammatical rules. Olena Shcherbakova and her colleagues focused on two measures of grammatical complexity: fusion (the merging of distinct phonemes) and informativity (the capacity for meaningful variation). They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"P1: The Linguistic Niche Hypothesis (LNH) suggests that as more non-native speakers adopt a language, its grammatical complexity decreases. This occurs because these speakers often do not fully acquire complex grammatical rules.\",\n",
        "    \"P2: Olena Shcherbakova and her colleagues focused on two measures of grammatical complexity: fusion (the merging of distinct phonemes) and informativity (the capacity for meaningful variation).\",\n",
        "    \"P3: They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’claim?\",\n",
        "    \"Ax) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and fusion, as well as a slightly negative correlation between grammatical complexity and informativity.\",\n",
        "    \"Bx) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and exotericity.\",\n",
        "    \"Cx) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical complexity and fusion.\",\n",
        "    \"Dv) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical fusion and exotericity, as well as a slightly positive correlation between grammatical informativity and exotericity.\"\n",
        "]"
      ],
      "metadata": {
        "id": "fmOVbZQsSw-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-3): LNH Study\n",
        "# This is the revised Example 3 to solve num-cont confusion in text.\n",
        "sentences = [\"Pa: The Linguistic Niche Hypothesis (LNH) suggests that as more non-native speakers adopt a language, its grammatical complexity decreases. This occurs because these speakers often do not fully acquire complex grammatical rules. Olena Shcherbakova and her colleagues focused on two measures of grammatical complexity: fusion (the merging of distinct phonemes) and informativity (the capacity for meaningful variation). They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"P1: The Linguistic Niche Hypothesis (LNH) suggests that as more non-native speakers adopt a language, its grammatical complexity decreases. This occurs because these speakers often do not fully acquire complex grammatical rules. Three initial factors.\",\n",
        "    \"P2: Olena Shcherbakova and her colleagues focused on two measures of grammatical complexity: fusion (the merging of distinct phonemes) and informativity (the capacity for meaningful variation). Add two additional factors, and three factors are studied.\",\n",
        "    \"P3: They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’claim?\",\n",
        "    \"Ax) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and fusion, as well as a slightly negative correlation between grammatical complexity and informativity. Three factors.\",\n",
        "    \"Bx) Shcherbakova and colleagues’ analysis showed a slightly negative correlation between grammatical complexity and exotericity. Two factors.\",\n",
        "    \"Cx) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical complexity and fusion. Two factors.\",\n",
        "    \"Dv) Shcherbakova and colleagues’ analysis showed a slightly positive correlation between grammatical fusion and exotericity, as well as a slightly positive correlation between grammatical informativity and exotericity. Three factors.\"\n",
        "]"
      ],
      "metadata": {
        "id": "VKfypVijrVku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-4): LNH Study\n",
        "# This is the revised Example 3 to solve mul-rel confusion in text by 5V.\n",
        "sentences = [\"Pa: LNH suggests that exotericity != complexity, and exotericity = non-native speakers. Olena Shcherbakova and her colleagues focused on two measures: 1) fusion = complexity, and 2) informativity = complexity. They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"P1: LNH suggests that exotericity != complexity, while exotericity = non-native speakers. Inital three factors \",\n",
        "    \"P2: Olena Shcherbakova and her colleagues focused on two measures: 1) fusion = complexity, and 2) informativity = complexity.\",\n",
        "    \"P3: They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’claim?\",\n",
        "    \"Ax) complexity != fusion, and complexity != informativity.\",\n",
        "    \"Bx) complexity != exotericity. Two factors.\",\n",
        "    \"Cx) complexity = fusion. Two factors.\",\n",
        "    \"Dv) fusion = exotericity, and informativity = exotericity.\"\n",
        "]"
      ],
      "metadata": {
        "id": "zhXAbtuk8Hs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 (Alt-5): LNH Study\n",
        "# This is the revised Example 3 to solve mul-rel and num-conut confusions in text by 5V.\n",
        "sentences = [\"Pa: LNH suggests that exotericity != complexity, and exotericity = non-native speakers. Olena Shcherbakova and her colleagues focused on two measures: 1) fusion = complexity, and 2) informativity = complexity. They analyzed over 1,300 languages and found their results contradict the LNH.\",\n",
        "    \"P1: LNH suggests that exotericity != complexity, while exotericity = non-native speakers. Inital three factors \",\n",
        "    \"P2: Olena Shcherbakova and her colleagues focused on two measures: 1) fusion = complexity, and 2) informativity = complexity. Three factors.\",\n",
        "    \"P3: They analyzed over 1,300 languages and found their results contradict the LNH. Five factors in total, only three factors were studied.\",\n",
        "    \"Q?: Which finding, if true, would most directly support Shcherbakova and colleagues’claim?\",\n",
        "    \"Ax) complexity != fusion, and complexity != informativity. Three factors.\",\n",
        "    \"Bx) complexity != exotericity. Two factors.\",\n",
        "    \"Cx) complexity = fusion. Two factors.\",\n",
        "    \"Dv) fusion = exotericity, and informativity = exotericity. Three factors.\"\n",
        "]"
      ],
      "metadata": {
        "id": "LcnHZBpaq6vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Gabler Ulysses\n",
        "# This is a hard question to discover long-comp confusion in text by 5V.\n",
        "sentences = [\"Pa: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"P1: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent.\",\n",
        "    \"P2: One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals.\",\n",
        "    \"P3: However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"Q?: Which choice most logically complete the text?\",\n",
        "    \"Ax) negative, since those Joyce scholars with the necessary expertise to write a review of the 1984 edition would be aware that facsimile manuscripts cannot be produced with a high enough fidelity to the original to ensure that relying on them will not introduce editorial errors.\",\n",
        "    \"Bx) positive, since scholars who reviewed the 1984 edition were unaffiliated with its production and were mostly either Joyce specialists who were largely unfamiliar with editorial theories and practices or specialists in such theories and practices who were insufficiently familiar with Joyce.\",\n",
        "    \"Cx) negative, since any scholar with expertise in editorial theories of the United States and United Kingdom as well as French and German editorial theories most likely worked with Gabler on the 1984 edition and would therefore not review it.\",\n",
        "    \"Dv) positive, since Ulysses is a novel in English and the 1984 edition would therefore be more widely reviewed in United States and United Kingdom publications than in French and German publications.\"\n",
        "]"
      ],
      "metadata": {
        "id": "7r-KxQSlgwZt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 (Alt-1): Gabler Ulysses\n",
        "# This is a hard question to discover irrelevance confusion in text by deleting V1(+/-).\n",
        "sentences = [\"Pa: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"P1: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent.\",\n",
        "    \"P2: One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals.\",\n",
        "    \"P3: However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"Q?: Which choice most logically complete the text?\",\n",
        "    \"Ax) since those Joyce scholars with the necessary expertise to write a review of the 1984 edition would be aware that facsimile manuscripts cannot be produced with a high enough fidelity to the original to ensure that relying on them will not introduce editorial errors.\",\n",
        "    \"Bx) since scholars who reviewed the 1984 edition were unaffiliated with its production and were mostly either Joyce specialists who were largely unfamiliar with editorial theories and practices or specialists in such theories and practices who were insufficiently familiar with Joyce.\",\n",
        "    \"Cx) since any scholar with expertise in editorial theories of the United States and United Kingdom as well as French and German editorial theories most likely worked with Gabler on the 1984 edition and would therefore not review it.\",\n",
        "    \"Dv) since Ulysses is a novel in English and the 1984 edition would therefore be more widely reviewed in United States and United Kingdom publications than in French and German publications.\"\n",
        "]"
      ],
      "metadata": {
        "id": "18Fdj1r82Z8F"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 (Alt-2): Gabler Ulysses\n",
        "# This is a hard question to mitigate long-comp confusion in text by 5V.\n",
        "sentences = [\"Pa: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"P1: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One; Contrast; Many; Negative.\",\n",
        "    \"P2: One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. One; Two; Contrast; Negative. Effect\",\n",
        "    \"P3: However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______. One; Negative; Many.\",\n",
        "    \"Q?: Which choice most logically complete the text?\",\n",
        "    \"Ax) negative, since those Joyce scholars with the necessary expertise to write a review of the 1984 edition would be aware that facsimile manuscripts cannot be produced with a high enough fidelity to the original to ensure that relying on them will not introduce editorial errors. Many; One; Negative; Positive. Effect\",\n",
        "    \"Bx) positive, since scholars who reviewed the 1984 edition were unaffiliated with its production and were mostly either Joyce specialists who were largely unfamiliar with editorial theories and practices or specialists in such theories and practices who were insufficiently familiar with Joyce. Many; One; Positive; Negative.\",\n",
        "    \"Cx) negative, since any scholar with expertise in editorial theories of the United States and United Kingdom as well as French and German editorial theories most likely worked with Gabler on the 1984 edition and would therefore not review it. Many; One; Two; Compare. Negative\",\n",
        "    \"Dv) positive, since Ulysses is a novel in English and the 1984 edition would therefore be more widely reviewed in United States and United Kingdom publications than in French and German publications. One; Two; Contrast. Positive.\"\n",
        "]"
      ],
      "metadata": {
        "id": "rkjGfDgKi7A7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 (Alt-3): Gabler Ulysses\n",
        "# This is a hard question to solve long-comp confusion in text by 5V.\n",
        "sentences = [\"Pa: The many editions of James Joyce's 1922 novel Ulysses are not textually identical, and scholars debate which versions reflect Joyce's authorial intent. One no longer widely read edition is the 1984 'critical and synoptic edition' edited by Hans Walter Gabler, which followed French and German editorial theories rather than editorial traditions of the United States and United Kingdom and which was later found to have introduced errors due to Gabler's choice to consult facsimile manuscripts rather than using only originals. However, few Joyce scholars worldwide had expertise in such textual issues, and most of those who did worked on the edition with Gabler. So, it is unsurprising that initial scholarly reviews of the 1984 edition were mostly _______.\",\n",
        "    \"P1: One; Contrast; Many; Negative.\",\n",
        "    \"P2: One; Negative Two; Contrast; Effect; Negative\",\n",
        "    \"P3: One; Negative; Many.\",\n",
        "    \"Q?: Which choice most logically match the text?\",\n",
        "    \"Ax) Many; One; Negative; Positive. Effect\",\n",
        "    \"Bx) Many; One; Positive; Negative.\",\n",
        "    \"Cx) Many; One; Two; Compare. Negative\",\n",
        "    \"Dv) One; Two; Contrast. Positive.\"\n",
        "]"
      ],
      "metadata": {
        "id": "RnQ18sirmf_v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: GRE Cotton Mother\n",
        "# This is a hard question GRE reading used to discover long-pass bias, long-comp confusion, num-cont confusion, and mul-rel confusion in text.\n",
        "sentences = [\"Pa: Among many historians a belief persists that Cotton Mather's biographies of some of the settlers of the Massachusetts Bay Colony (published 1702) are exercises in hagiography, endowing their subjects with saintly piety at the expense of historical accuracy. Yet modern studies have profited both from the breadth of information that Mather provides in, for example, his discussions of colonial medicine and from his critical observations of such leading figures as Governor John Winthrop. Mather's wry humor as demonstrated by his detailed descriptions of events such as Winthrop's efforts to prevent wood-stealing is overlooked by those charging Mather with presenting his subjects as extremely pious. The charge also obscures Mather's concern with the settlers material, not just spiritual, prosperity. Further, this pejorative view underrates the biographies value as chronicles: Mather amassed all sorts of published and unpublished documents as sources, and his selection of key eventsshows a marked sensitivity to the nature of the colony's development.\",\n",
        "    \"P1: Among many historians a belief persists that Cotton Mather's biographies of some of the settlers of the Massachusetts Bay Colony (published 1702) are exercises in hagiography, endowing their subjects with saintly piety at the expense of historical accuracy.\",\n",
        "    \"P2: Yet modern studies have profited both from the breadth of information that Mather provides in, for example, his discussions of colonial medicine and from his critical observations of such leading figures as Governor John Winthrop.\",\n",
        "    \"P3: Mather's wry humor as demonstrated by his detailed descriptions of events such as Winthrop's efforts to prevent wood-stealing is overlooked by those charging Mather with presenting his subjects as extremely pious.\",\n",
        "    \"P4: The charge also obscures Mather's concern with the settlers material, not just spiritual, prosperity. (P5:) Further, this pejorative view underrates the biographies value as chronicles: Mather amassed all sorts of published and unpublished documents as sources, and his selection of key eventsshows a marked sensitivity to the nature of the colony's development.\",\n",
        "    \"Av) The primary purpose of the passage is to argue against a theory universally accepted by historical researchers\",\n",
        "    \"Bx) The primary purpose of the passage is to call attention to an unusual approach to documenting a historical era\",\n",
        "    \"Cx) The primary purpose of the passage is to summarize research on a specific historical figure\",\n",
        "    \"Dx) The primary purpose of the passage is to counter a particular view about the work of a biographer\",\n",
        "    \"Ex) The primary purpose of the passage is to point out subtle differences among controversial historical reports\"\n",
        "]"
      ],
      "metadata": {
        "id": "tRBeMYKlMRbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 (Alt-1): GRE Cotton Mother\n",
        "# This is the revised Example 3 to solve long-pass confusion and spec-confusion in graduate level text.\n",
        "sentences = [\"Pa: Among many historians a belief persists that Cotton Mather's biographies of some of the settlers of the Massachusetts Bay Colony (published 1702) are exercises in hagiography, endowing their subjects with saintly piety at the expense of historical accuracy. Yet modern studies have profited both from the breadth of information that Mather provides in, for example, his discussions of colonial medicine and from his critical observations of such leading figures as Governor John Winthrop. Mather's wry humor as demonstrated by his detailed descriptions of events such as Winthrop's efforts to prevent wood-stealing is overlooked by those charging Mather with presenting his subjects as extremely pious. The charge also obscures Mather's concern with the settlers material, not just spiritual, prosperity. Further, this pejorative view underrates the biographies value as chronicles: Mather amassed all sorts of published and unpublished documents as sources, and his selection of key eventsshows a marked sensitivity to the nature of the colony's development.\",\n",
        "    \"P1: Many believed that Cotton Mather's biographies of settlers of Massachusetts Bay Colony are exercises in hagiography.\",\n",
        "    \"P2: Yet modern studies have profited both from information that Mather provides and from his critical observations of leading figures.\",\n",
        "    \"P3: Mather's wry humor is overlooked by those charging Mather with presenting his subjects as extremely pious.\",\n",
        "    \"P4: The charge also obscures Mather's concern with the settlers material. (P5:) Further, this pejorative view underrates the biographies value as chronicles.\",\n",
        "    \"Av) The primary purpose of the passage is to argue against a theory universally accepted by historical researchers\",\n",
        "    \"Bx) The primary purpose of the passage is to call attention to an unusual approach to documenting a historical era\",\n",
        "    \"Cx) The primary purpose of the passage is to summarize research on a specific historical figure\",\n",
        "    \"Dx) The primary purpose of the passage is to counter a particular view about the work of a biographer\",\n",
        "    \"Ex) The primary purpose of the passage is to point out subtle differences among controversial historical reports\"\n",
        "]"
      ],
      "metadata": {
        "id": "Vio4gQQvT8BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 (Alt-2): GRE Cotton Mother\n",
        "# This is the revised Example 3 to mitigate spec-confusions in text by 5V.\n",
        "sentences = [\"Pa: Among many historians a belief persists that Cotton Mather's biographies of some of the settlers of the Massachusetts Bay Colony (published 1702) are exercises in hagiography, endowing their subjects with saintly piety at the expense of historical accuracy. Yet modern studies have profited both from the breadth of information that Mather provides in, for example, his discussions of colonial medicine and from his critical observations of such leading figures as Governor John Winthrop. Mather's wry humor as demonstrated by his detailed descriptions of events such as Winthrop's efforts to prevent wood-stealing is overlooked by those charging Mather with presenting his subjects as extremely pious. The charge also obscures Mather's concern with the settlers material, not just spiritual, prosperity. Further, this pejorative view underrates the biographies value as chronicles: Mather amassed all sorts of published and unpublished documents as sources, and his selection of key eventsshows a marked sensitivity to the nature of the colony's development.\",\n",
        "    \"P1: Many believed that Cotton Mather's biographies of settlers of Massachusetts Bay Colony are exercises in hagiography. One; Subjective.\",\n",
        "    \"P2: Yet modern studies have profited both from information that Mather provides and from his critical observations of leading figures. Two; Objective. Positive.\",\n",
        "    \"P3: Mather's wry humor is overlooked by those charging Mather with presenting his subjects as extremely pious. One; Objective; Negative.\",\n",
        "    \"P4: The charge also obscures Mather's concern with the settlers material. One; Negative; Subjective. (P5:) Further, this pejorative view underrates the biographies value as chronicles. One; Subjective; Negative.\",\n",
        "    \"Ax) The primary purpose of the passage is to argue against a theory universally accepted by historical researchers. One; Many; Objective.\",\n",
        "    \"Bx) The primary purpose of the passage is to call attention to an unusual approach to documenting a historical era. One; Many; Subjective.\",\n",
        "    \"Cx) The primary purpose of the passage is to summarize research on a specific historical figure. One; Subjective.\",\n",
        "    \"Dv) The primary purpose of the passage is to counter a particular view about the work of a biographer. One; Subjective; Negative\",\n",
        "    \"Ex) The primary purpose of the passage is to point out subtle differences among controversial historical reports. Two; Negative; Objective; Contrast.\"\n",
        "]"
      ],
      "metadata": {
        "id": "kNdkUoJgVpNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 (Alt-3): GRE Cotton Mother\n",
        "# This is the revised Example 3 to solve spec-confusions in text by 5V.\n",
        "sentences = [\"Pa: One; Subjective; Negative.\",\n",
        "    \"P1: Many: One; Subjective.\",\n",
        "    \"P2: Two; Objective. Positive.\",\n",
        "    \"P3: One; Objective; Negative.\",\n",
        "    \"P4: One; Negative; Subjective. One; Subjective; Negative.\",\n",
        "    \"Ax) One; Many; Objective.\",\n",
        "    \"Bx) One; Many; Subjective.\",\n",
        "    \"Cx) One; Subjective.\",\n",
        "    \"Dv) One; Subjective; Negative\",\n",
        "    \"Ex) Two; Negative; Objective; Contrast.\"\n",
        "]"
      ],
      "metadata": {
        "id": "AM49t10yx-l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate Embeddings for CeO2 Sample Sentences.\n",
        "\n",
        "Used a pre-trained Sentence Transformer model, `all-mpnet-base-v2`, to generate embeddings for each sentence. This model is a good general-purpose choice, with mapping of sentences & paragraphs to a **768** dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
        "\n",
        "`all-mpnet-base-v2`: is a sentence transformer model that converts sentences and paragraphs into numerical vectors. These vectors, called embeddings, capture the semantic meaning of the text.\n",
        "\n",
        "**Key Features and What it's Used For**:\n",
        "* Sentence Embeddings: The model takes a sentence (or short paragraph) as input and produces a dense vector representation.\n",
        "* 768 Dimensions: is a common dimensionality for these types of models.\n",
        "* MPNet Architecture: is a model that combines the strengths of BERT and XLNet to better understand word order and context.\n",
        "* General Purpose: `all-mpnet-base-v2` is designed to be a general-purpose model, meaning it performs well on a variety of tasks.\n",
        "\n",
        "**Common Applications**:\n",
        "* Semantic Search: Finding sentences or documents with similar meanings.\n",
        "* Information Retrieval: Pulling up relevant information based on a text query.\n",
        "* Clustering: Grouping sentences or paragraphs with similar meanings.\n",
        "* Sentence Similarity: Measuring how alike two pieces of text are.\n",
        "\n",
        "**How to Use It** (in Python with Sentence Transformers):\n",
        "\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model = SentenceTransformer('all-mpnet-base-v2')\n",
        "        sentences = [\n",
        "        \"This is a simple example.\",\n",
        "        \"Here is another sentence.\",\n",
        "        \"A third sentence for demonstration.\"\n",
        "    ]\n",
        "    embeddings = model.encode(sentences)\n",
        "    print(embeddings)  # Output: A list of 768-dimensional vectors\n",
        "\n",
        "**Why is it Popular?**\n",
        "* Inclusive dataset: a massive dataset of over 1 billion sentence pairs from\n",
        "Natural Language Inference (NLI) datasets, Paraphrase datasets, and a large collection of other published English data from various sources.\n",
        "* Good Performance: It generally ranks high in accuracy for many semantic text understanding tasks.\n",
        "* Efficiency: While very effective, it's also relatively efficient to use.\n",
        "* Ease of Use: Libraries like Sentence Transformers make it very easy to download and use."
      ],
      "metadata": {
        "id": "mFQ_kUZAA5RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Generate the embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "print(\"Shape of embeddings:\", embeddings.shape)\n",
        "print(\"Example embedding (first sentence):\\n\", embeddings[8][:100]) # Print the first 20 dimensions only"
      ],
      "metadata": {
        "id": "O_TbU_wZBkSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Reduce Dimensionality: 2D with PCA\n",
        "\n",
        "**PCA** = principal component analysis.\n",
        "\n",
        "Principal Components are composite dimensions from the existing 768-dimention embedding dataset. It helps to reduce dimensions while maintaning the most variances mathematically, thus being able to discern the most of datapoints. The ability to discern things is simply the intelligence, or smartness."
      ],
      "metadata": {
        "id": "ocb8gsESB1VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce to 2 dimensions using PCA\n",
        "pca_2d = PCA(n_components=2)\n",
        "reduced_embeddings_2d = pca_2d.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the 2D embeddings\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.scatter(reduced_embeddings_2d[:, 0], reduced_embeddings_2d[:, 1])\n",
        "\n",
        "# Annotate each point with the corresponding sentence\n",
        "for i, txt in enumerate(sentences):\n",
        "    plt.annotate(txt[:2], (reduced_embeddings_2d[i, 0], reduced_embeddings_2d[i, 1]))\n",
        "\n",
        "plt.title(\"2D Visualization of Sentence Embeddings (PCA)\")\n",
        "plt.xlabel(\"PC 1\")\n",
        "plt.ylabel(\"PC 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fJeP42tQCDmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Reduce Dimensionality: 3D with PCA\n",
        "\n",
        "### 6.1 3D PCA static"
      ],
      "metadata": {
        "id": "uc8WbutRCRBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce to 3 dimensions using PCA\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the 3D embeddings\n",
        "fig_3d = plt.figure(figsize=(8, 6))\n",
        "ax_3d = fig_3d.add_subplot(111, projection='3d')\n",
        "scatter_3d = ax_3d.scatter(reduced_embeddings_3d[:, 0], reduced_embeddings_3d[:, 1], reduced_embeddings_3d[:, 2])\n",
        "\n",
        "# Annotate each point with the corresponding sentence\n",
        "for i, txt in enumerate(sentences):\n",
        "    ax_3d.text(reduced_embeddings_3d[i, 0], reduced_embeddings_3d[i, 1], reduced_embeddings_3d[i, 2], txt[:2])\n",
        "\n",
        "ax_3d.set_xlabel(\"PC 1\")\n",
        "ax_3d.set_ylabel(\"PC 2\")\n",
        "ax_3d.set_zlabel(\"PC 3\")\n",
        "ax_3d.set_title(\"3D Visualization of Sentence Embeddings (PCA)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fySiUjFOCa9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 3D PCA rotational with P-tet\n",
        "\n",
        "Question domain is illustrated in P-tet. Choices relation to Prompt-Question is shown by the distance from Choice to P-tet."
      ],
      "metadata": {
        "id": "dnOCVriiwN5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "labels = [s[:2] for s in sentences]  # Get first two letters.\n",
        "\n",
        "# Perform PCA to reduce to 3 dimensions\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "# Directly define the indices, assuming P1, P2, P3, Q? are at indices 1, 2, 3, and 4\n",
        "p1_index = 1\n",
        "p2_index = 2\n",
        "p3_index = 3\n",
        "q_index = 4\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_embeddings_3d[p1_index]\n",
        "p2_coords = reduced_embeddings_3d[p2_index]\n",
        "p3_coords = reduced_embeddings_3d[p3_index]\n",
        "q_coords = reduced_embeddings_3d[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}'\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Create the 3D scatter plot and add the lines\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=reduced_embeddings_3d[:, 0],\n",
        "    y=reduced_embeddings_3d[:, 1],\n",
        "    z=reduced_embeddings_3d[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,  # Use first 2 characters of labels\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=6),\n",
        "    name='Data Points'\n",
        ")] + lines)  # Combine scatter and lines\n",
        "\n",
        "# Set the title and axis labels\n",
        "fig.update_layout(\n",
        "    title=\"3D PCA Visualization with Tetrahedron\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hEAM8I15WJ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 P-sphere, P-tet, and Scatterplot"
      ],
      "metadata": {
        "id": "ZeBZ5VgiqmWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sphere, tetrahedron, and scatterplot.\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "labels = [s[:2] for s in sentences]  # Get first two letters.\n",
        "\n",
        "# Perform PCA to reduce to 3 dimensions\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "# Directly define the indices, assuming P1, P2, P3, Q? are at indices 1, 2, 3, and 4\n",
        "p1_index = 1\n",
        "p2_index = 2\n",
        "p3_index = 3\n",
        "q_index = 4\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_embeddings_3d[p1_index]\n",
        "p2_coords = reduced_embeddings_3d[p2_index]\n",
        "p3_coords = reduced_embeddings_3d[p3_index]\n",
        "q_coords = reduced_embeddings_3d[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}'\n",
        "        )\n",
        "    )\n",
        "\n",
        "# --- Start of new code for sphere calculation ---\n",
        "# Points for sphere calculation\n",
        "P = [p1_coords, p2_coords, p3_coords, q_coords]\n",
        "x = [p[0] for p in P]\n",
        "y = [p[1] for p in P]\n",
        "z = [p[2] for p in P]\n",
        "\n",
        "# System of equations to find sphere center (cx, cy, cz)\n",
        "# (xi-cx)^2 + (yi-cy)^2 + (zi-cz)^2 = R^2\n",
        "# Subtracting equation for P1 from P2, P3, P4:\n",
        "# 2(x2-x1)cx + 2(y2-y1)cy + 2(z2-z1)cz = (x2^2+y2^2+z2^2) - (x1^2+y1^2+z1^2)\n",
        "# ... and so on for P3 and P4\n",
        "\n",
        "A = np.array([\n",
        "    [2*(x[1]-x[0]), 2*(y[1]-y[0]), 2*(z[1]-z[0])],\n",
        "    [2*(x[2]-x[0]), 2*(y[2]-y[0]), 2*(z[2]-z[0])],\n",
        "    [2*(x[3]-x[0]), 2*(y[3]-y[0]), 2*(z[3]-z[0])]\n",
        "])\n",
        "\n",
        "B = np.array([\n",
        "    (x[1]**2 + y[1]**2 + z[1]**2) - (x[0]**2 + y[0]**2 + z[0]**2),\n",
        "    (x[2]**2 + y[2]**2 + z[2]**2) - (x[0]**2 + y[0]**2 + z[0]**2),\n",
        "    (x[3]**2 + y[3]**2 + z[3]**2) - (x[0]**2 + y[0]**2 + z[0]**2)\n",
        "])\n",
        "\n",
        "try:\n",
        "    sphere_center = np.linalg.solve(A, B)\n",
        "    cx, cy, cz = sphere_center[0], sphere_center[1], sphere_center[2]\n",
        "    sphere_radius = np.sqrt((x[0]-cx)**2 + (y[0]-cy)**2 + (z[0]-cz)**2)\n",
        "\n",
        "    # Generate sphere surface points\n",
        "    u = np.linspace(0, 2 * np.pi, 50) # Azimuthal angle\n",
        "    v = np.linspace(0, np.pi, 25)    # Polar angle\n",
        "\n",
        "    sphere_x = cx + sphere_radius * np.outer(np.cos(u), np.sin(v))\n",
        "    sphere_y = cy + sphere_radius * np.outer(np.sin(u), np.sin(v))\n",
        "    sphere_z = cz + sphere_radius * np.outer(np.ones(np.size(u)), np.cos(v)) # Corrected from np.ones_like(u) to np.ones(np.size(u))\n",
        "\n",
        "    sphere_surface = go.Surface(\n",
        "        x=sphere_x, y=sphere_y, z=sphere_z,\n",
        "        opacity=0.3,\n",
        "        colorscale='Blues', # You can choose other colorscales e.g. 'Viridis', 'RdBu'\n",
        "        showscale=False, # Hide the color scale bar for the sphere\n",
        "        name='Sphere'\n",
        "    )\n",
        "    sphere_added = True\n",
        "except np.linalg.LinAlgError:\n",
        "    print(\"Could not determine the sphere: Points might be coplanar or collinear.\")\n",
        "    sphere_surface = None # No sphere to add if calculation fails\n",
        "    sphere_added = False\n",
        "# --- End of new code for sphere calculation ---\n",
        "\n",
        "# Create the 3D scatter plot and add the lines and sphere\n",
        "data_elements = [go.Scatter3d(\n",
        "    x=reduced_embeddings_3d[:, 0],\n",
        "    y=reduced_embeddings_3d[:, 1],\n",
        "    z=reduced_embeddings_3d[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,  # Use first 2 characters of labels\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4),\n",
        "    name='Data Points'\n",
        ")] + lines\n",
        "\n",
        "if sphere_added and sphere_surface:\n",
        "    data_elements.append(sphere_surface)\n",
        "\n",
        "fig = go.Figure(data=data_elements)\n",
        "\n",
        "\n",
        "# Set the title and axis labels\n",
        "fig.update_layout(\n",
        "    title=\"3D PCA Visualization with Tetrahedron and Sphere\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "        # Aspect ratio to make the sphere look more like a sphere\n",
        "        aspectmode='data' # 'auto', 'cube', 'data', 'manual'\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6OJW4SC5ARB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 3D PCA from multiple angles static\n",
        "\n",
        "In case rotational view above isn't available at temp environment, use multi-angle to illustrate."
      ],
      "metadata": {
        "id": "iGJWUAOfxByM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA from multiple angles static views\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "#Create various angles for multiple 3-D rendering\n",
        "elevations = [30, 30, 0, -30]  # Angles of elevation\n",
        "azim_angles = [0, 45, 90, 135] # Azimuthal angles\n",
        "\n",
        "for i, (elev, azim) in enumerate(zip(elevations, azim_angles)):\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(reduced_embeddings_3d[:, 0], reduced_embeddings_3d[:, 1], reduced_embeddings_3d[:, 2])\n",
        "    for j, txt in enumerate(sentences):\n",
        "        ax.text(reduced_embeddings_3d[j, 0], reduced_embeddings_3d[j, 1], reduced_embeddings_3d[j, 2], txt[:2])\n",
        "    ax.set_xlabel(\"PC 1\")\n",
        "    ax.set_ylabel(\"PC 2\")\n",
        "    ax.set_zlabel(\"PC 3\")\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    plt.title(f\"3D Plot (Elev={elev}, Azim={azim})\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qzJJQ2qd59hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Using t-SNE for Dimensionality Reduction\n",
        "\n",
        "**t-SNE** = t-distributed stochastic neighbor embedding. Unlike **PCA**, **t-SNE** demonstrate the embeddings in non-linear more scattered fashion.\n"
      ],
      "metadata": {
        "id": "E2N5RVTwCs8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 2D t-SNE"
      ],
      "metadata": {
        "id": "ZKd0J7HaxlFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce to 2 dimensions using t-SNE with a lower perplexity\n",
        "tsne_2d = TSNE(n_components=2, random_state=42, n_iter=300, perplexity=min(5, len(sentences) - 1)) # Set perplexity <= 5 or n_samples - 1\n",
        "reduced_embeddings_tsne_2d = tsne_2d.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the 2D embeddings (t-SNE)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(reduced_embeddings_tsne_2d[:, 0], reduced_embeddings_tsne_2d[:, 1])\n",
        "\n",
        "# Annotate each point with the corresponding sentence\n",
        "for i, txt in enumerate(sentences):\n",
        "    plt.annotate(txt[:2], (reduced_embeddings_tsne_2d[i, 0], reduced_embeddings_tsne_2d[i, 1]))\n",
        "\n",
        "plt.title(\"2D Visualization of Sentence Embeddings (t-SNE)\")\n",
        "plt.xlabel(\"t-SNE D1\")\n",
        "plt.ylabel(\"t-SNE D2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8lgTh7bOEX6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.2 3D t-SNE static"
      ],
      "metadata": {
        "id": "NgbblG8Mxsy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Reduce to 3 dimensions using t-SNE with a lower perplexity\n",
        "tsne_3d = TSNE(n_components=3, random_state=42, n_iter=300, perplexity=min(5, len(sentences) - 1))\n",
        "reduced_embeddings_tsne_3d = tsne_3d.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the 3D embeddings\n",
        "fig_3d = plt.figure(figsize=(4, 3))\n",
        "ax_3d = fig_3d.add_subplot(111, projection='3d')\n",
        "scatter_3d = ax_3d.scatter(reduced_embeddings_tsne_3d[:, 0], reduced_embeddings_tsne_3d[:, 1], reduced_embeddings_tsne_3d[:, 2])\n",
        "\n",
        "# Annotate each point with the first letter of the sentence\n",
        "for i, txt in enumerate(sentences):\n",
        "    ax_3d.text(reduced_embeddings_tsne_3d[i, 0], reduced_embeddings_tsne_3d[i, 1], reduced_embeddings_tsne_3d[i, 2], txt[:2])\n",
        "\n",
        "ax_3d.set_xlabel(\"t-SNE D1\")\n",
        "ax_3d.set_ylabel(\"t-SNE D2\")\n",
        "ax_3d.set_zlabel(\"t-SNE D3\")\n",
        "ax_3d.set_title(\"3D Visualization of Sentence Embeddings (t-SNE)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VoIIlz1E3e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.3 3D t-SNE rotational"
      ],
      "metadata": {
        "id": "UinugAfyxyO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming you have your data in a variable called 'embeddings'\n",
        "# For example, let's create some dummy data for demonstration:\n",
        "embeddings = np.random.rand(9, 5)  # 9 samples, 5 dimensions\n",
        "sentences = [\n",
        "    \"Pa: Some fuel additives\",\n",
        "    \"P1: Some fuel additives\",\n",
        "    \"P2: In a 2015 study\",\n",
        "    \"P3: While bioaccumulation\",\n",
        "    \"Q?: Which finding\",\n",
        "    \"Ax) When D. polymorpha\",\n",
        "    \"Bv) The rate of CeO2-NPs\",\n",
        "    \"Cx) D.polymorpha has been\",\n",
        "    \"Dx) Compared with O. mykiss\"\n",
        "]\n",
        "labels = [s[:2] for s in sentences]  # Get first two letters.\n",
        "\n",
        "# Perform PCA to reduce to 3 dimensions\n",
        "pca_3d = PCA(n_components=3)\n",
        "reduced_embeddings_tsne_3d = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "# Directly define the indices, assuming P1, P2, P3, Q? are at indices 1, 2, 3, and 4\n",
        "p1_index = 1\n",
        "p2_index = 2\n",
        "p3_index = 3\n",
        "q_index = 4\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_embeddings_tsne_3d[p1_index]\n",
        "p2_coords = reduced_embeddings_tsne_3d[p2_index]\n",
        "p3_coords = reduced_embeddings_tsne_3d[p3_index]\n",
        "q_coords = reduced_embeddings_tsne_3d[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}'\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Create the 3D scatter plot and add the lines\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=reduced_embeddings_tsne_3d[:, 0],\n",
        "    y=reduced_embeddings_tsne_3d[:, 1],\n",
        "    z=reduced_embeddings_tsne_3d[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,  # Use first 2 characters of labels\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=8),\n",
        "    name='Data Points'\n",
        ")] + lines)  # Combine scatter and lines\n",
        "\n",
        "# Set the title and axis labels\n",
        "fig.update_layout(\n",
        "    title=\"3D PCA Visualization with Tetrahedron\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "Q6Sj8TzBJLyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. How about 5-Vs?\n",
        "\n",
        "### 8.1 Initialized 5V (CL 1wk + DR 1wk)\n",
        "\n",
        "Below is a typical preception with inital training/ While erros exist across V2/V3/V4/V5, the student can vaguely see the correct answer Bv from the wrong choices Ax, Cx, and Dx.\n",
        "\n",
        "5Vs---Pa---P1---P2---P3---Q?---Ax---Bv---Cx---Dx---\n",
        "\n",
        "V1-----0-----1-----0------0-----1------0-----1-----0-----1---\n",
        "\n",
        "V2-----1-----0----0.5---1------0-----0----0.5----0-----0---\n",
        "\n",
        "V3-----1-----0----0.5---1-----1------1-----1-----1-----1---\n",
        "\n",
        "V4-----0-----1-----0-----0-----1------0-----1----0.5----1---\n",
        "\n",
        "V5-----0-----1-----0----0.5----0------0-----0-----0-----1---"
      ],
      "metadata": {
        "id": "1Bdv-muoBmlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot and P-Tet: 5V-ini\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,0\n",
        "P1,0,0,0,1,1\n",
        "P2,0,0.5,0.5,0,0\n",
        "P3,0,1,1,0,0.5\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0.5,1,1,0\n",
        "Cx,0,0,1,0.5,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "from io import StringIO\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# Get the indices of P1, P2, P3, and Q?\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the 3D scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0],\n",
        "    y=reduced_data[:, 1],\n",
        "    z=reduced_data[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4),\n",
        "    name='Data Points'  # Add a name for the scatter plot\n",
        ")\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}' # Add a name for each line.\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Combine scatter and lines\n",
        "data = [scatter] + lines\n",
        "\n",
        "# Set the title and axis labels\n",
        "layout = go.Layout(\n",
        "    title=\"3D PCA of Data with Tetrahedron\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        "    showlegend=True #show legend\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "# Show the plot (this will display an interactive plot in Colab)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "5e3crrvNJ33O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot, P-Tet, and P-sphere: 5V-ini\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,0\n",
        "P1,0,0,0,1,1\n",
        "P2,0,0.5,0.5,0,0\n",
        "P3,0,1,1,0,0.5\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0.5,1,1,0\n",
        "Cx,0,0,1,0.5,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# Get the indices of P1, P2, P3, and Q?\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron (also the points defining the sphere)\n",
        "sphere_defining_points = [p1_coords, p2_coords, p3_coords, q_coords]\n",
        "tetrahedron_vertices = np.array(sphere_defining_points)\n",
        "\n",
        "# --- Functions to calculate sphere center and radius ---\n",
        "def get_sphere_coeffs(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the coefficients A, B, C, D of the sphere equation:\n",
        "    x^2 + y^2 + z^2 + Ax + By + Cz + D = 0\n",
        "    passing through four points p1, p2, p3, p4.\n",
        "    \"\"\"\n",
        "    points = np.array([p1, p2, p3, p4])\n",
        "    # Form matrix M for the system M * [A, B, C, D]' = -[x^2+y^2+z^2]'\n",
        "    M = np.ones((4, 4))\n",
        "    M[:, 0] = points[:, 0]  # x coordinates\n",
        "    M[:, 1] = points[:, 1]  # y coordinates\n",
        "    M[:, 2] = points[:, 2]  # z coordinates\n",
        "\n",
        "    # Right hand side vector\n",
        "    rhs = -(points[:, 0]**2 + points[:, 1]**2 + points[:, 2]**2)\n",
        "\n",
        "    try:\n",
        "        # Solve for A, B, C, D\n",
        "        coeffs = np.linalg.solve(M, rhs)\n",
        "        return coeffs\n",
        "    except np.linalg.LinAlgError:\n",
        "        print(\"Error: The four points might be coplanar. Cannot determine a unique sphere.\")\n",
        "        return None\n",
        "\n",
        "def calculate_sphere_center_radius(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the center and radius of a sphere passing through four points.\n",
        "    p1, p2, p3, p4 are 3D points as NumPy arrays or lists [x, y, z].\n",
        "    Returns (center, radius) or (None, None) if points are coplanar.\n",
        "    \"\"\"\n",
        "    coeffs = get_sphere_coeffs(np.array(p1), np.array(p2), np.array(p3), np.array(p4))\n",
        "\n",
        "    if coeffs is None:\n",
        "        return None, None\n",
        "\n",
        "    A, B, C, D_coeff = coeffs\n",
        "\n",
        "    # Center (xc, yc, zc)\n",
        "    xc = -A / 2\n",
        "    yc = -B / 2\n",
        "    zc = -C / 2\n",
        "    center = np.array([xc, yc, zc])\n",
        "\n",
        "    # Radius R\n",
        "    # R^2 = xc^2 + yc^2 + zc^2 - D_coeff\n",
        "    radius_sq = xc**2 + yc**2 + zc**2 - D_coeff\n",
        "    if radius_sq < 0:\n",
        "        print(\"Error: Calculated radius squared is negative (points might be collinear or an issue with PCA reduction).\")\n",
        "        return None, None\n",
        "    radius = np.sqrt(radius_sq)\n",
        "\n",
        "    return center, radius\n",
        "\n",
        "# --- Function to generate Plotly sphere surface ---\n",
        "def get_plotly_sphere_surface(center, radius, color='rgba(0,180,255,0.3)', resolution=50, name='Circumsphere'):\n",
        "    \"\"\"\n",
        "    Generates Plotly go.Surface data for a sphere.\n",
        "    center: NumPy array or list for the sphere's center [xc, yc, zc].\n",
        "    radius: Radius of the sphere.\n",
        "    color: Color of the sphere as an rgba string (e.g., 'rgba(R,G,B,A)').\n",
        "    resolution: Number of points for theta and phi.\n",
        "    name: Legend name for the sphere.\n",
        "    \"\"\"\n",
        "    theta = np.linspace(0, 2 * np.pi, resolution)\n",
        "    phi = np.linspace(0, np.pi, resolution)\n",
        "    theta, phi = np.meshgrid(theta, phi)\n",
        "\n",
        "    x = center[0] + radius * np.cos(theta) * np.sin(phi)\n",
        "    y = center[1] + radius * np.sin(theta) * np.sin(phi)\n",
        "    z = center[2] + radius * np.cos(phi)\n",
        "\n",
        "    # To achieve a single color with opacity, we set the color directly in the colorscale\n",
        "    # and ensure opacity is handled by the color string itself or the opacity property of go.Surface\n",
        "\n",
        "    # Extract RGB from the rgba string if provided, default to a blue if format is unexpected\n",
        "    try:\n",
        "        rgb_color_part = color.split('(')[1].split(')')[0].split(',')\n",
        "        r, g, b = rgb_color_part[0], rgb_color_part[1], rgb_color_part[2]\n",
        "        plotly_color = f'rgb({r},{g},{b})'\n",
        "        opacity_val = float(rgb_color_part[3]) if len(rgb_color_part) > 3 else 0.3\n",
        "    except:\n",
        "        plotly_color = 'rgb(0,180,255)' # Default blue\n",
        "        opacity_val = 0.3\n",
        "\n",
        "    return go.Surface(\n",
        "        x=x, y=y, z=z,\n",
        "        colorscale=[[0, plotly_color], [1, plotly_color]], # Solid color\n",
        "        showscale=False,\n",
        "        opacity=opacity_val,\n",
        "        name=name,\n",
        "        hoverinfo='skip' # Optional: disable hover for the sphere surface\n",
        "    )\n",
        "\n",
        "# --- Plotting ---\n",
        "\n",
        "# Create the 3D scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0],\n",
        "    y=reduced_data[:, 1],\n",
        "    z=reduced_data[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4, color='black'), # Changed marker color for better visibility against sphere\n",
        "    name='Data Points'\n",
        ")\n",
        "\n",
        "# Define the edges of the tetrahedron\n",
        "tetrahedron_edges_indices = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1\n",
        "    (1, 2), (1, 3),          # Edges from P2\n",
        "    (2, 3)                   # Edge from P3\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_idx, end_idx) in enumerate(tetrahedron_edges_indices):\n",
        "    start_point = tetrahedron_vertices[start_idx]\n",
        "    end_point = tetrahedron_vertices[end_idx]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=3),\n",
        "            name=f'Tetrahedron Edge' # Simplified name\n",
        "        )\n",
        "    )\n",
        "# To avoid multiple \"Tetrahedron Edge\" legends, only the first one will show by default if names are identical.\n",
        "# Or, make them unique if needed, or group them. For simplicity, keep as is or assign name to only one.\n",
        "if lines:\n",
        "    lines[0].showlegend = True # Show legend for the first edge only as representative\n",
        "    for line_trace in lines[1:]:\n",
        "        line_trace.showlegend = False\n",
        "\n",
        "\n",
        "# Calculate sphere center and radius using the PCA-reduced coordinates\n",
        "sphere_center, sphere_radius = calculate_sphere_center_radius(p1_coords, p2_coords, p3_coords, q_coords)\n",
        "\n",
        "# Initialize data list for the figure\n",
        "data_traces = [scatter] + lines\n",
        "\n",
        "# Add sphere to the plot if calculation was successful\n",
        "sphere_trace = None\n",
        "if sphere_center is not None and sphere_radius is not None:\n",
        "    print(f\"Sphere Center (PCA coords): {sphere_center}\")\n",
        "    print(f\"Sphere Radius (PCA coords): {sphere_radius}\")\n",
        "    sphere_trace = get_plotly_sphere_surface(\n",
        "        sphere_center,\n",
        "        sphere_radius,\n",
        "        color='rgba(100, 180, 255, 0.3)', # Light blue, semi-transparent\n",
        "        resolution=40, # Lower resolution for faster rendering, increase for smoother sphere\n",
        "        name='Circumsphere P1-P2-P3-Q?'\n",
        "    )\n",
        "    data_traces.append(sphere_trace)\n",
        "else:\n",
        "    print(\"Could not calculate sphere parameters. Sphere will not be plotted.\")\n",
        "\n",
        "\n",
        "# Set the title and axis labels\n",
        "layout = go.Layout(\n",
        "    title=\"3D PCA: Data Points, Tetrahedron (P1-P2-P3-Q?), and Circumsphere\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "        aspectmode='data' # 'data', 'cube', 'auto', 'manual'\n",
        "                         # 'data' ensures that the scaling of axes matches the data range\n",
        "                         # 'cube' makes the plot region a cube\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=data_traces, layout=layout)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "HLd_OFwiUZwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Well-Trained 5Vs: CL 1wk + DP/BT>=1month\n",
        "\n",
        "Below is a typical preception with 1+month training. Correct answer falls directly into the prompt/question subspace.\n",
        "\n",
        "5Vs---Pa---P1---P2---P3---Q?---Ax---Bv---Cx---Dx---\n",
        "\n",
        "V1-----0-----1-----0------0-----1------0-----1-----0-----1---\n",
        "\n",
        "V2-----1-----0-----1-----1------0------0-----0-----0-----0---\n",
        "\n",
        "V3-----1-----0-----1------1-----1------1-----1-----1-----1---\n",
        "\n",
        "V4-----0-----1-----0-----0-----1------0------1-----1-----1---\n",
        "\n",
        "V5-----0-----1-----0-----1-----0------0------0-----0-----1---"
      ],
      "metadata": {
        "id": "GiE1M_gtFaj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaterplot and P-Tet: 5V-fin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Your data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,1\n",
        "P1,1,0,0,1,1\n",
        "P2,0,1,1,0,0\n",
        "P3,0,1,1,0,1\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0,1,1,0\n",
        "Cx,0,0,1,1,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "from io import StringIO\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# Get the indices of P1, P2, P3, and Q?\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron\n",
        "tetrahedron_vertices = np.array([p1_coords, p2_coords, p3_coords, q_coords])\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices)\n",
        "tetrahedron_edges = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1 to P2, P3, Q?\n",
        "    (1, 2), (1, 3),          # Edges from P2 to P3, Q?\n",
        "    (2, 3)                   # Edge from P3 to Q?\n",
        "]\n",
        "\n",
        "# Create the 3D scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0],\n",
        "    y=reduced_data[:, 1],\n",
        "    z=reduced_data[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4),\n",
        "    name='Data Points'  # Add a name for the scatter plot\n",
        ")\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_index, end_index) in enumerate(tetrahedron_edges):\n",
        "    start_point = tetrahedron_vertices[start_index]\n",
        "    end_point = tetrahedron_vertices[end_index]\n",
        "    lines.append(\n",
        "        go.Scatter3d(\n",
        "            x=[start_point[0], end_point[0]],\n",
        "            y=[start_point[1], end_point[1]],\n",
        "            z=[start_point[2], end_point[2]],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=4),  # Style the lines\n",
        "            name=f'Edge {i+1}' # Add a name for each line.\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Combine scatter and lines\n",
        "data = [scatter] + lines\n",
        "\n",
        "# Set the title and axis labels\n",
        "layout = go.Layout(\n",
        "    title=\"3D PCA of Data with Tetrahedron\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        "    showlegend=True #show legend\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "# Show the plot (this will display an interactive plot in Colab)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ampbw-BTFY7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaterplot, P-Tet, and P-sphere: 5V-fin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "\n",
        "# Your new data as a string (including header row)\n",
        "data_string = \"\"\"datapoint,V1,V2,V3,V4,V5\n",
        "Pa,0,1,1,0,1\n",
        "P1,1,0,0,1,1\n",
        "P2,0,1,1,0,0\n",
        "P3,0,1,1,0,1\n",
        "Q?,1,0,1,1,0\n",
        "Ax,0,0,1,0,0\n",
        "Bv,1,0,1,1,0\n",
        "Cx,0,0,1,1,0\n",
        "Dx,1,0,1,1,1\n",
        "\"\"\"\n",
        "\n",
        "# Load the data from the string into a Pandas DataFrame\n",
        "df = pd.read_csv(StringIO(data_string))\n",
        "\n",
        "# Extract the data for PCA (exclude the 'datapoint' column)\n",
        "X = df.iloc[:, 1:].values  # Get values from columns V1 to V5\n",
        "\n",
        "# Extract the datapoint labels\n",
        "labels = df['datapoint'].tolist()\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "reduced_data = pca.fit_transform(X)\n",
        "\n",
        "# Get the indices of P1, P2, P3, and Q?\n",
        "p1_index = labels.index('P1')\n",
        "p2_index = labels.index('P2')\n",
        "p3_index = labels.index('P3')\n",
        "q_index = labels.index('Q?')\n",
        "\n",
        "# Extract the 3D coordinates of P1, P2, P3, and Q?\n",
        "p1_coords = reduced_data[p1_index]\n",
        "p2_coords = reduced_data[p2_index]\n",
        "p3_coords = reduced_data[p3_index]\n",
        "q_coords = reduced_data[q_index]\n",
        "\n",
        "# Define the vertices of the tetrahedron (also the points defining the sphere)\n",
        "sphere_defining_points = [p1_coords, p2_coords, p3_coords, q_coords]\n",
        "tetrahedron_vertices = np.array(sphere_defining_points)\n",
        "\n",
        "\n",
        "# --- Functions to calculate sphere center and radius ---\n",
        "def get_sphere_coeffs(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the coefficients A, B, C, D of the sphere equation:\n",
        "    x^2 + y^2 + z^2 + Ax + By + Cz + D = 0\n",
        "    passing through four points p1, p2, p3, p4.\n",
        "    \"\"\"\n",
        "    points = np.array([p1, p2, p3, p4])\n",
        "    M = np.ones((4, 4))\n",
        "    M[:, 0] = points[:, 0]  # x coordinates\n",
        "    M[:, 1] = points[:, 1]  # y coordinates\n",
        "    M[:, 2] = points[:, 2]  # z coordinates\n",
        "    rhs = -(points[:, 0]**2 + points[:, 1]**2 + points[:, 2]**2)\n",
        "    try:\n",
        "        coeffs = np.linalg.solve(M, rhs)\n",
        "        return coeffs\n",
        "    except np.linalg.LinAlgError:\n",
        "        print(\"Error: The four points might be coplanar. Cannot determine a unique sphere.\")\n",
        "        return None\n",
        "\n",
        "def calculate_sphere_center_radius(p1, p2, p3, p4):\n",
        "    \"\"\"\n",
        "    Calculates the center and radius of a sphere passing through four points.\n",
        "    Returns (center, radius) or (None, None) if points are coplanar.\n",
        "    \"\"\"\n",
        "    coeffs = get_sphere_coeffs(np.array(p1), np.array(p2), np.array(p3), np.array(p4))\n",
        "    if coeffs is None:\n",
        "        return None, None\n",
        "    A, B, C, D_coeff = coeffs\n",
        "    xc = -A / 2\n",
        "    yc = -B / 2\n",
        "    zc = -C / 2\n",
        "    center = np.array([xc, yc, zc])\n",
        "    radius_sq = xc**2 + yc**2 + zc**2 - D_coeff\n",
        "    if radius_sq < 0:\n",
        "        # Check if points are nearly collinear/coplanar leading to large A,B,C and thus large xc,yc,zc\n",
        "        # This can sometimes happen with PCA reduced data if variance is very low in one dimension\n",
        "        # or if points are numerically challenging.\n",
        "        # A very small positive radius_sq might also be an issue if it's due to precision.\n",
        "        print(f\"Warning: Calculated radius squared is negative or very small ({radius_sq}). Points might be problematic.\")\n",
        "        # Allow small positive radius, but flag if strictly negative\n",
        "        if radius_sq < -1e-9: # Tolerance for floating point errors\n",
        "             print(\"Error: Strictly negative radius squared. Cannot form sphere.\")\n",
        "             return None, None\n",
        "        elif radius_sq < 0: # If slightly negative, treat as zero radius (problematic point configuration)\n",
        "            print(\"Adjusting negative radius squared to zero for calculation, but the sphere will be a point.\")\n",
        "            radius_sq = 0\n",
        "\n",
        "    radius = np.sqrt(radius_sq)\n",
        "    return center, radius\n",
        "\n",
        "# --- Function to generate Plotly sphere surface ---\n",
        "def get_plotly_sphere_surface(center, radius, color='rgba(0,180,255,0.3)', resolution=50, name='Circumsphere'):\n",
        "    \"\"\"\n",
        "    Generates Plotly go.Surface data for a sphere.\n",
        "    \"\"\"\n",
        "    theta = np.linspace(0, 2 * np.pi, resolution)\n",
        "    phi = np.linspace(0, np.pi, resolution)\n",
        "    theta, phi = np.meshgrid(theta, phi)\n",
        "    x = center[0] + radius * np.cos(theta) * np.sin(phi)\n",
        "    y = center[1] + radius * np.sin(theta) * np.sin(phi)\n",
        "    z = center[2] + radius * np.cos(phi)\n",
        "\n",
        "    try:\n",
        "        rgb_color_part = color.split('(')[1].split(')')[0].split(',')\n",
        "        r, g, b = rgb_color_part[0], rgb_color_part[1], rgb_color_part[2]\n",
        "        plotly_color = f'rgb({r},{g},{b})'\n",
        "        opacity_val = float(rgb_color_part[3]) if len(rgb_color_part) > 3 else 0.3\n",
        "    except:\n",
        "        plotly_color = 'rgb(0,180,255)' # Default blue\n",
        "        opacity_val = 0.3\n",
        "\n",
        "    return go.Surface(\n",
        "        x=x, y=y, z=z,\n",
        "        colorscale=[[0, plotly_color], [1, plotly_color]],\n",
        "        showscale=False,\n",
        "        opacity=opacity_val,\n",
        "        name=name,\n",
        "        hoverinfo='skip'\n",
        "    )\n",
        "\n",
        "# --- Plotting ---\n",
        "\n",
        "# Create the 3D scatter plot for all points\n",
        "scatter = go.Scatter3d(\n",
        "    x=reduced_data[:, 0],\n",
        "    y=reduced_data[:, 1],\n",
        "    z=reduced_data[:, 2],\n",
        "    mode='markers+text',\n",
        "    text=labels,\n",
        "    textposition=\"middle right\",\n",
        "    marker=dict(size=4, color='black'), # Adjusted size and color\n",
        "    name='Data Points'\n",
        ")\n",
        "\n",
        "# Define the edges of the tetrahedron (indices of vertices in tetrahedron_vertices)\n",
        "tetrahedron_edges_indices = [\n",
        "    (0, 1), (0, 2), (0, 3),  # Edges from P1\n",
        "    (1, 2), (1, 3),          # Edges from P2\n",
        "    (2, 3)                   # Edge from P3\n",
        "]\n",
        "\n",
        "# Create the lines for the tetrahedron edges\n",
        "lines = []\n",
        "for i, (start_idx, end_idx) in enumerate(tetrahedron_edges_indices):\n",
        "    start_point = tetrahedron_vertices[start_idx]\n",
        "    end_point = tetrahedron_vertices[end_idx]\n",
        "    trace = go.Scatter3d(\n",
        "        x=[start_point[0], end_point[0]],\n",
        "        y=[start_point[1], end_point[1]],\n",
        "        z=[start_point[2], end_point[2]],\n",
        "        mode='lines',\n",
        "        line=dict(color='magenta', width=3), # Changed color for variety\n",
        "        name='Tetrahedron Edge'\n",
        "    )\n",
        "    # Show legend only for the first edge trace to avoid clutter\n",
        "    if i == 0:\n",
        "        trace.showlegend = True\n",
        "    else:\n",
        "        trace.showlegend = False\n",
        "    lines.append(trace)\n",
        "\n",
        "# Calculate sphere center and radius using the PCA-reduced coordinates\n",
        "sphere_center, sphere_radius = calculate_sphere_center_radius(p1_coords, p2_coords, p3_coords, q_coords)\n",
        "\n",
        "# Initialize data list for the figure\n",
        "data_traces = [scatter] + lines\n",
        "\n",
        "# Add sphere to the plot if calculation was successful\n",
        "sphere_trace = None\n",
        "if sphere_center is not None and sphere_radius is not None:\n",
        "    if np.isnan(sphere_center).any() or np.isnan(sphere_radius):\n",
        "        print(\"Sphere calculation resulted in NaN values. Sphere will not be plotted.\")\n",
        "    elif sphere_radius < 1e-6 : # If radius is effectively zero\n",
        "        print(f\"Sphere radius is very small ({sphere_radius}). Plotting as a point at center {sphere_center} (or skipping).\")\n",
        "        # Optionally, plot a marker for the center if radius is too small\n",
        "        # data_traces.append(go.Scatter3d(x=[sphere_center[0]], y=[sphere_center[1]], z=[sphere_center[2]],\n",
        "        #                                  mode='markers', marker=dict(size=5, color='white'), name='SCtr'))\n",
        "    else:\n",
        "        print(f\"Sphere Center (PCA coords): {sphere_center}\")\n",
        "        print(f\"Sphere Radius (PCA coords): {sphere_radius}\")\n",
        "        sphere_trace = get_plotly_sphere_surface(\n",
        "            sphere_center,\n",
        "            sphere_radius,\n",
        "            color='rgba(100, 180, 255, 0.3)', # blueish, semi-transparent\n",
        "            resolution=40,\n",
        "            name='Circumsphere (P1-P2-P3-Q?)'\n",
        "        )\n",
        "        data_traces.append(sphere_trace)\n",
        "else:\n",
        "    print(\"Could not calculate sphere parameters (e.g., coplanar points). Sphere will not be plotted.\")\n",
        "\n",
        "\n",
        "# Set the title and axis labels\n",
        "layout = go.Layout(\n",
        "    title=\"3D PCA: Data Points, Tetrahedron, and Circumsphere\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC 1\",\n",
        "        yaxis_title=\"PC 2\",\n",
        "        zaxis_title=\"PC 3\",\n",
        "        aspectmode='data' # Options: 'data', 'cube', 'auto', 'manual'\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=0),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=data_traces, layout=layout)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0lTtD-1vV4Wl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "LIT in Notebooks",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}